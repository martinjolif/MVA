{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# Coding a MaskGIT autoencoder in Pytorch\n",
        "\n",
        "For any remark or suggestion on this lab, please feel free to contact me at:\n",
        "*   loic dot lefolgoc at telecom-paris dot fr.\n",
        "\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to __explore VQ-VAEs and MaskGIT__, a generative image transformer model operating on tokenized representations (as typically output by VQ-VAEs). We will design a MaskGIT autoencoder applied to MNIST.\n",
        "\n",
        "## Your task:\n",
        "You need to __add the missing parts in the code__ (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE or ...).<br>\n",
        "\n",
        "You can train models for a few epochs on CPU or GPU to check that your code works. To save your time, __I actually provide models trained for longer for all parts of the lab__, which you can load once the relevant code is completed, before moving on to the next part.\n",
        "\n",
        "__Please upload the lab by the deadline as a .ipynb or a .zip named SURNAME_FirstName.ipynb or .zip__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let's load some packages:"
      ],
      "metadata": {
        "id": "gp13aVUQq1WX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "U6NKzRPlDKZp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vector-quantize-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvLPB3e0sHPp",
        "outputId": "030292d5-97a0-4cbf-af4a-b513f995c6d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vector-quantize-pytorch in /usr/local/lib/python3.11/dist-packages (1.21.9)\n",
            "Requirement already satisfied: einops>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from vector-quantize-pytorch) (0.8.1)\n",
            "Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from vector-quantize-pytorch) (0.3.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from vector-quantize-pytorch) (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->vector-quantize-pytorch) (1.26.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->vector-quantize-pytorch) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.11/dist-packages (from einx>=0.3.0->vector-quantize-pytorch) (2.4.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->vector-quantize-pytorch) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->einx>=0.3.0->vector-quantize-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->vector-quantize-pytorch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDfGfVdbv2BK",
        "outputId": "a0a0c901-c885-4c37-c7e2-8fb27604c5f2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from einops import rearrange, repeat"
      ],
      "metadata": {
        "id": "9z4suXPhv4SN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vector_quantize_pytorch import VectorQuantize"
      ],
      "metadata": {
        "id": "-MzjBKW3rmqL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Loading MNIST"
      ],
      "metadata": {
        "id": "0zXGp6SpmD3q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "First, we __load the mnist dataset__. You should be able to train for a couple of epochs on the full training dataset `mnist_trainset` even on CPU, but should you need it, we create a reduced trainset below.\n",
        "\n",
        "Feel free to train on `mnist_trainset_reduced` instead if you prefer. To do so, replace the argument `mnist_trainset` in the `torch.utils.data.DataLoader(...)` call creating `mnist_train_loader` in the cell below by `mnist_trainset_reduced` (and same for `mnist_testset` and `mnist_testset_reduced`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4YPLKlPrufSk"
      },
      "outputs": [],
      "source": [
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create reduced datasets for quick experimentation\n",
        "\n",
        "# train\n",
        "max_mnist_size = 10000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]\n",
        "\n",
        "# test\n",
        "max_mnist_size = 1000\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0]"
      ],
      "metadata": {
        "id": "4_JahnGp-PBP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64"
      ],
      "metadata": {
        "id": "uxj8ZwAN1Hw3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the dataloaders\n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "StRXmUUE2iIJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7YhlBT2PN9I",
        "outputId": "4d0b0b25-ec64-438c-8a48-ebcc9f027e00"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "mnist_trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. VQ-VAE"
      ],
      "metadata": {
        "id": "Ba07bfUmoPbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of the lab is about __training a VQ-VAE model__. We will abstract details about quantization and commitment loss by using an existing library that takes care of this for us via the `VectorQuantize` layer. We will however have to build the encoding-decoding architecture around it.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9KPruYTMnwTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Building the VQ-VAE"
      ],
      "metadata": {
        "id": "6vSVaAQPkkW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The VQ-VAE model consists of\n",
        "*   an encoder, which maps from the original image space $H_0\\times W_0\\times 1$ to a lower-resolution image space $h\\times w\\times D$\n",
        "*   a `VectorQuantize` layer, which does the quantization i.e., turns the output of the encoder to $h\\times w$ images of integer tokens. It is also able to compute the commitment_loss, which is added to the reconstruction loss when training. Finally, it also returns quantized embeddings $h\\times w\\times D$ (a quantized version of the encoder output, which serves as the decoder input).\n",
        "*   a decoder, which goes back from quantized embeddings $h\\times w\\times D$ to an image $H_0\\times W_0\\times 1$\n",
        "\n",
        "The encoder/decoder consist of down/upsampling layers (conv or transpose conv with stride 2) and a residual stack to further process the feature maps."
      ],
      "metadata": {
        "id": "FYr_-2lvkjgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualStack(nn.Module):\n",
        "    '''\n",
        "    The ResidualStack is already coded for you.\n",
        "    '''\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super().__init__()\n",
        "        # See Section 4.1 of \"Neural Discrete Representation Learning\".\n",
        "        layers = []\n",
        "        for i in range(num_residual_layers):\n",
        "            layers.append(\n",
        "                nn.Sequential(\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_hiddens,\n",
        "                        out_channels=num_residual_hiddens,\n",
        "                        kernel_size=3,\n",
        "                        padding=1,\n",
        "                    ),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(\n",
        "                        in_channels=num_residual_hiddens,\n",
        "                        out_channels=num_hiddens,\n",
        "                        kernel_size=1,\n",
        "                    ),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        :args:\n",
        "          x -> FloatTensor(), shape (C, H, W): the input\n",
        "        :return:\n",
        "          FloatTensor(), shape (C, H, W): the output of the same shape.\n",
        "        '''\n",
        "        h = x\n",
        "        for layer in self.layers:\n",
        "            h = h + layer(h)\n",
        "\n",
        "        # ResNet V1-style.\n",
        "        return torch.relu(h)"
      ],
      "metadata": {
        "id": "Wm9NrUUxoTkT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's code the `Encoder`. It consists of `self.conv`, a `nn.Sequential()` object with `num_downsampling_layers` downsampling layers, which are convolutions with\n",
        "*   stride of 2\n",
        "*   kernel size: 4x4\n",
        "*   padding: 1\n",
        "\n",
        "and a final conv (k:3x3 padding:1, stride:1) which outputs an image with `num_hidden` channels; this is followed by a `ResidualStack` with `num_residual_layers` and internally `num_residual_hiddens` channels.\n",
        "\n"
      ],
      "metadata": {
        "id": "P-ML9fzizVlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        num_hiddens,\n",
        "        num_downsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        conv = nn.Sequential()\n",
        "        for downsampling_layer in range(num_downsampling_layers):\n",
        "            if downsampling_layer == 0:\n",
        "                out_channels = num_hiddens // 2\n",
        "            elif downsampling_layer == 1:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, num_hiddens)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            # Add the Conv2d layer\n",
        "            conv.add_module(\n",
        "                f\"down{downsampling_layer}\",\n",
        "                nn.Conv2d(in_channels, out_channels, (4,4), 2, 1),\n",
        "            )\n",
        "            conv.add_module(f\"relu{downsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        # Add the final Conv2d layer\n",
        "        conv.add_module(\n",
        "            \"final_conv\",\n",
        "            nn.Conv2d(out_channels, num_hiddens, (3,3), 1, 1),\n",
        "        )\n",
        "        self.conv = conv\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        return self.residual_stack(h)"
      ],
      "metadata": {
        "id": "AqzFXofkoaVp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Encoder class by running this cell\n",
        "\n",
        "# The input shape is (5, 1, 28, 28), the output shape should be (5, 10, 7, 7).\n",
        "\n",
        "temp = torch.randn(5, 1, 28, 28)\n",
        "temp = Encoder(in_channels = 1,\n",
        "               num_hiddens = 10,\n",
        "               num_downsampling_layers = 2,\n",
        "               num_residual_layers = 2,\n",
        "               num_residual_hiddens = 5\n",
        "               )(temp)\n",
        "temp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo6xtYnh2B2T",
        "outputId": "aec4948d-9a5d-4219-a988-86e4e92f113e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 10, 7, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we code the `Decoder`. It starts with a `self.conv` layer, a `Conv2d` layer which projects from (embedding_dim, h, w) to (num_hidden, h, w). Then a `ResidualStack` follows to mirror the encoder. Then upsampling `ConvTranspose2d` layers listed in a `nn.Sequential()` object, with:\n",
        "*   kernel size: 4x4\n",
        "*   stride: 2\n",
        "*   padding: 1\n",
        "\n"
      ],
      "metadata": {
        "id": "2R2x_ZAW3BUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim,\n",
        "        num_hiddens,\n",
        "        num_upsampling_layers,\n",
        "        num_residual_layers,\n",
        "        num_residual_hiddens,\n",
        "        x_channels\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv = nn.Conv2d(embedding_dim, num_hiddens, (3,3), 1, 1)\n",
        "\n",
        "        self.residual_stack = ResidualStack(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "        upconv = nn.Sequential()\n",
        "        for upsampling_layer in range(num_upsampling_layers):\n",
        "            if upsampling_layer < num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens)\n",
        "\n",
        "            elif upsampling_layer == num_upsampling_layers - 2:\n",
        "                (in_channels, out_channels) = (num_hiddens, num_hiddens // 2)\n",
        "\n",
        "            else:\n",
        "                (in_channels, out_channels) = (num_hiddens // 2, x_channels)\n",
        "\n",
        "            # Add the ConvTranspose2d\n",
        "            upconv.add_module(\n",
        "                f\"up{upsampling_layer}\",\n",
        "                nn.ConvTranspose2d(in_channels, out_channels, (4,4), 2, 1)\n",
        "            )\n",
        "            if upsampling_layer < num_upsampling_layers - 1:\n",
        "                upconv.add_module(f\"relu{upsampling_layer}\", nn.ReLU())\n",
        "\n",
        "        self.upconv = upconv\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        h = self.residual_stack(h)\n",
        "        x_recon = self.upconv(h)\n",
        "        return x_recon"
      ],
      "metadata": {
        "id": "MwG_afqhpfKT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the Decoder class by running this cell\n",
        "\n",
        "# The input shape is (5, 5, 7, 7), the output shape should be (5, 1, 28, 28).\n",
        "\n",
        "temp = torch.randn(5, 5, 7, 7)\n",
        "temp = Decoder(embedding_dim = 5,\n",
        "               num_hiddens = 10,\n",
        "               num_upsampling_layers = 2,\n",
        "               num_residual_layers = 2,\n",
        "               num_residual_hiddens = 5,\n",
        "               x_channels = 1\n",
        "               )(temp)\n",
        "temp.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Is8wPlC_4m57",
        "outputId": "216a3688-0e59-4980-d5ee-9c0c8e6276a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `VQVAE` class wraps it all together: `Encoder`, `VectorQuantize` layer, `Decoder`.\n",
        "* The `forward` will be called at training time. It attempts to reconstruct its input (auto-encoding). It relies on a call to the `quantize` routine.\n",
        "* At inference time, we will call the `encode` and `decode` routines instead, which respectively let us obtain the (integer) image tokens, and reconstruct an image from input tokens.<br>\n",
        "\n",
        "Fill the missing parts (more details provided in the code)."
      ],
      "metadata": {
        "id": "fpuHJJhGVqR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main calls to interact with the `VectorQuantize` layer (say, an instance `vq`) are:\n",
        "* `z_quantized, encoding_indices, commitment_loss = vq(z)`, where `z` are D-dimensional embeddings output by an encoder, that are compared to `K` D-dimensional codes in a codebook\n",
        "* `z_quantized = vq.get_output_from_indices(encoding_indices)` in case you start from a grid of tokens and want to compute the corresponding quantized embeddings; the output format for this routine and this routine only is `(b, h, w, d)` instead of the `(b, d, h, w)` required by most layers (like convolutional layers), so that a rearrange will be needed afterwards."
      ],
      "metadata": {
        "id": "0XyEEDlQirl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VQVAE(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,              # number of input channels\n",
        "        num_hiddens,              # cf. Encoder and Decoder\n",
        "        num_downsampling_layers,  # if set to 2, downsample factor is 4\n",
        "        num_residual_layers,      # cf. Encoder and Decoder\n",
        "        num_residual_hiddens,     # cf. ResidualStack\n",
        "        embedding_dim,            # the length of a vector in the codebook (D)\n",
        "        codebook_size,            # the size of the codebook (K)\n",
        "        decay = 0.8,              # technical parameter (for EMA updates of the codebook), lower means the dictionary will change faster\n",
        "        commitment_weight = 1.,   # the weight of the commitment_loss compared to the reconstruction loss\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # class Encoder()\n",
        "        self.encoder = Encoder(in_channels, num_hiddens, num_downsampling_layers, num_residual_layers, num_residual_hiddens)\n",
        "\n",
        "        # An additional Conv2d called right after the encoder to project from\n",
        "        # num_hidden channels to embedding_dim channels (so that embeddings\n",
        "        # can be compared to the codes in the codebook).\n",
        "        self.pre_vq_conv = nn.Conv2d(\n",
        "            in_channels=num_hiddens, out_channels=embedding_dim, kernel_size=1\n",
        "        )\n",
        "\n",
        "        # The quantization layer\n",
        "        self.vq = VectorQuantize(\n",
        "            dim = embedding_dim,\n",
        "            codebook_size = codebook_size,\n",
        "            decay = decay,\n",
        "            commitment_weight = commitment_weight,\n",
        "            accept_image_fmap = True  # overrides the default, which is to deal with 1D signals\n",
        "        )\n",
        "\n",
        "        # class Decoder()\n",
        "        self.decoder = Decoder(embedding_dim, num_hiddens, num_downsampling_layers, num_residual_layers, num_residual_hiddens, 1)\n",
        "\n",
        "    def quantize(self, x):\n",
        "        '''\n",
        "        :args\n",
        "            x -> FloatTensor() - (b, c, H0, W0): the batch of input images\n",
        "        :return\n",
        "            z_quantized -> FloatTensor() - (b, D, h, w): the quantized embeddings\n",
        "            commitment_loss -> FloatTensor() - (b,): the commitment loss value for all samples in the minibatch. cf. VQ-VAE paper\n",
        "            encoding_indices -> LongTensor() - (b, h, w): the grid of (integer) tokens\n",
        "        '''\n",
        "\n",
        "        # Pass through encoder, then through pre_vq_conv\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "\n",
        "        # Pass through the vector quantization layer\n",
        "        z_quantized, encoding_indices, commitment_loss = self.vq(z)    # outputs are (b, h, w, d), (b, h, w), (b)\n",
        "\n",
        "        return (z_quantized, commitment_loss, encoding_indices)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encode and quantize\n",
        "        (z_quantized, commitment_loss, _) = self.quantize(x)\n",
        "\n",
        "        # Reconstruct by passing through the decoder z_quantized\n",
        "        x_rec = self.decoder(z_quantized)\n",
        "\n",
        "        return {\n",
        "            \"commitment_loss\": commitment_loss,\n",
        "            \"x_rec\": x_rec,\n",
        "        }\n",
        "\n",
        "    def encode(self, x):\n",
        "        z = self.pre_vq_conv(self.encoder(x))\n",
        "        z_quantized, encoding_indices, _ = self.vq(z)\n",
        "\n",
        "        return encoding_indices, z_quantized, z\n",
        "\n",
        "    def decode(self, code_indices):\n",
        "        # Generate z_quantized from code_indices (see hint in the text above this cell)\n",
        "        z_quantized = self.vq.get_output_from_indices(code_indices)\n",
        "        z_quantized = rearrange(z_quantized, 'b h w d -> b d h w')\n",
        "\n",
        "        return self.decoder(z_quantized)"
      ],
      "metadata": {
        "id": "Nrpp4xK7rgRI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the VQVAE class by running this cell\n",
        "\n",
        "temp = torch.randn(2, 1, 28, 28)\n",
        "vq_temp = VQVAE(\n",
        "    in_channels = 1,\n",
        "    num_hiddens = 10,\n",
        "    num_downsampling_layers = 2,\n",
        "    num_residual_layers = 2,\n",
        "    num_residual_hiddens = 5,\n",
        "    embedding_dim = 5,\n",
        "    codebook_size = 100,\n",
        "    decay = 0.9,\n",
        "    commitment_weight = 1.\n",
        ")\n",
        "\n",
        "temp_o = vq_temp(temp)\n",
        "print('Reconstructed shape: {}'.format(temp_o['x_rec'].shape)) # should be (2, 1, 28, 28)\n",
        "\n",
        "temp_tokens, temp_z_quantized, _ = vq_temp.encode(temp)\n",
        "print('Tokens shape: {}'.format(temp_tokens.shape)) # should be (2, 7, 7)\n",
        "print('Quantized embeddings shape: {}'.format(temp_z_quantized.shape)) # should be (2, 5, 7, 7)\n",
        "\n",
        "temp_o = vq_temp.decode(temp_tokens)\n",
        "print('Reconstructed shape: {}'.format(temp_o.shape)) # should be (2, 1, 28, 28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alxVTtvGkwhB",
        "outputId": "b78795d7-4884-40fa-b477-93c40ebfefe2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed shape: torch.Size([2, 1, 28, 28])\n",
            "Tokens shape: torch.Size([2, 7, 7])\n",
            "Quantized embeddings shape: torch.Size([2, 5, 7, 7])\n",
            "Reconstructed shape: torch.Size([2, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Training the VQ-VAE"
      ],
      "metadata": {
        "id": "HTQ7erh1xbWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2636fa7c-c9cd-4a12-a66e-7c5b569e24b2",
        "id": "PXq1SIZRxbWJ"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training proceeds as usual. We instantiate a model, move it to the correct device, create an optimizer and write the training loop."
      ],
      "metadata": {
        "id": "dXeiSYrgxbWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "in_channels = 1\n",
        "num_hiddens = 64\n",
        "num_downsampling_layers = 2\n",
        "num_residual_layers = 2\n",
        "num_residual_hiddens = 32\n",
        "embedding_dim = 32\n",
        "codebook_size = 64\n",
        "\n",
        "learning_rate = 1e-4\n",
        "n_epoch = 10 # if running on GPU you can use more epochs"
      ],
      "metadata": {
        "id": "UbKCt1Y9xbWJ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Mi4PR_BTxbWK"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "vqvae_model = VQVAE(\n",
        "    in_channels,\n",
        "    num_hiddens,\n",
        "    num_downsampling_layers,\n",
        "    num_residual_layers,\n",
        "    num_residual_hiddens,\n",
        "    embedding_dim,\n",
        "    codebook_size,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model = vqvae_model.to(device)"
      ],
      "metadata": {
        "id": "1N1yM4NA_kHJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the AdamW optimizer, set the correct learning rate and weight_decay to 1e-4\n",
        "optimizer = optim.AdamW(vqvae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "i418nwfLxbWK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us write the training loop. We should use a reconstruction loss adapted to binary masks. Note that the decoder of VQVAE ends with a `Conv2d` rather than a `sigmoid`, which means that it outputs logits rather than probabilities. __Which reconstruction loss is suitable?__\n",
        "* `F.mse_loss`\n",
        "* `F.cross_entropy`\n",
        "* `F.binary_cross_entropy`\n",
        "* `F.binary_cross_entropy_with_logits`\n",
        "\n",
        "You can use `reduction=\"mean\"`."
      ],
      "metadata": {
        "id": "PiZSdS4TntVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device, GPU or CPU\n",
        "      data = data.to(device)\n",
        "\n",
        "      # Pass the input data through the model\n",
        "      predict = vqvae_model(data)\n",
        "      x_hat = predict['x_rec']\n",
        "\n",
        "      # Compute the VQ-VAE loss\n",
        "      reconstruction_loss = F.binary_cross_entropy_with_logits(x_hat, data, reduction='mean')\n",
        "      commitment_loss = predict['commitment_loss'].mean()\n",
        "      loss = reconstruction_loss + commitment_loss\n",
        "\n",
        "      # Backpropagate\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Aggregate the training loss for display at the end of the epoch\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16af36d6-8e1f-4e7e-af00-2ce5a53fd62e",
        "id": "Pi8odIz9xbWK"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [00:27<00:00, 34.53batch/s, loss=0.0834]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 0.1693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [00:19<00:00, 48.49batch/s, loss=0.0757]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 0.0812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [00:18<00:00, 49.47batch/s, loss=0.0728]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 0.0767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [00:19<00:00, 48.58batch/s, loss=0.0697]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 0.0748\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [00:19<00:00, 49.11batch/s, loss=0.0705]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 0.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 938/938 [00:20<00:00, 46.38batch/s, loss=0.0787]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 0.0729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 938/938 [00:19<00:00, 48.98batch/s, loss=0.0677]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss: 0.0724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 938/938 [00:19<00:00, 49.00batch/s, loss=0.0778]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss: 0.0721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 938/938 [00:19<00:00, 48.27batch/s, loss=0.066]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss: 0.0718\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 938/938 [00:18<00:00, 49.41batch/s, loss=0.0783]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss: 0.0717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you think the training goes in the right direction, hence your model is correct, you can actually __load an already trained VQVAE__ if you prefer. If your code is correct and if you kept the same model parameters, it should load without failure. Note that __if you want to use the already trained AE-MaskGIT model in the second part of the lab__ below, you __have to__ use this VQVAE that I provide you with."
      ],
      "metadata": {
        "id": "W3GBjoxfo8p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1E5uK7xccsiOoaUpClNcd5vAdZAfmXtCd' -O weights_vqvae.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn6MaAvFCyxT",
        "outputId": "12821a92-ac2f-4635-8f02-8f8420175dbf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2025-03-04 17:21:11--  https://drive.google.com/uc?export=download&id=1E5uK7xccsiOoaUpClNcd5vAdZAfmXtCd\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.135.113, 74.125.135.138, 74.125.135.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.135.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1E5uK7xccsiOoaUpClNcd5vAdZAfmXtCd&export=download [following]\n",
            "--2025-03-04 17:21:11--  https://drive.usercontent.google.com/download?id=1E5uK7xccsiOoaUpClNcd5vAdZAfmXtCd&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 855686 (836K) [application/octet-stream]\n",
            "Saving to: ‘weights_vqvae.pth’\n",
            "\n",
            "weights_vqvae.pth   100%[===================>] 835.63K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2025-03-04 17:21:14 (92.9 MB/s) - ‘weights_vqvae.pth’ saved [855686/855686]\n",
            "\n",
            "FINISHED --2025-03-04 17:21:14--\n",
            "Total wall clock time: 2.9s\n",
            "Downloaded: 1 files, 836K in 0.009s (92.9 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dict = torch.load('weights_vqvae.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUqCbEg8DCV-",
        "outputId": "4c85efde-110c-4dd4-84eb-dc4563fa3da3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-2b4f9cbf993e>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights_dict = torch.load('weights_vqvae.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model = VQVAE(in_channels = in_channels,\n",
        "                    num_hiddens = num_hiddens,\n",
        "                    num_downsampling_layers = num_downsampling_layers,\n",
        "                    num_residual_layers = num_residual_layers,\n",
        "                    num_residual_hiddens = num_residual_hiddens,\n",
        "                    embedding_dim = embedding_dim,\n",
        "                    codebook_size = codebook_size,\n",
        "                    decay = 0.95)"
      ],
      "metadata": {
        "id": "HDaCM-9XDc-3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model.load_state_dict(weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP9HY7-CDC1_",
        "outputId": "a5144a37-0c52-4150-aaac-1fd6dee7fbb7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model = vqvae_model.to(device)"
      ],
      "metadata": {
        "id": "YRQrf4jsDFDG"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Done, let's move to testing the model!"
      ],
      "metadata": {
        "id": "hUtCqtuopkXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Testing the VQ-VAE"
      ],
      "metadata": {
        "id": "tKQ2bU-_xbWK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwczqCQVxbWK"
      },
      "source": [
        "We define functions for qualitative testing of the autoencoder model. We will reuse them throughout the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dIynOXfcxbWK"
      },
      "outputs": [],
      "source": [
        "def display_images(imgs):\n",
        "  '''\n",
        "  Display a batch of images (typically synthetic/generated images)\n",
        "  '''\n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    # black and white images\n",
        "    axs[j].imshow(imgs[j, 0,:,:].detach().cpu().numpy(), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs, device):\n",
        "  '''\n",
        "  Display a batch of input images along with their reconstructions by a given model\n",
        "    First row: input images\n",
        "    Second row: reconstructed images\n",
        "  '''\n",
        "\n",
        "  ae_model.eval()\n",
        "\n",
        "  # choose random images\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "\n",
        "  # get reconstructions\n",
        "  # Don't forget to pass the output of vq_vae through a sigmoid to get images!\n",
        "  output_imgs = torch.sigmoid(ae_model(test_imgs.to(device))['x_rec'])\n",
        "  output_imgs = output_imgs.detach().cpu().numpy()\n",
        "\n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well the autoencoder reconstructs images from the training set:"
      ],
      "metadata": {
        "id": "egTZ-5PTxbWK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "c630d61f-09b6-44f0-8ad4-b7f6a2530646",
        "id": "2vszQFBKxbWK"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAEzCAYAAABOlRseAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAFw9JREFUeJzt3WuwlWXZB/AbBUVBQOSgIpWahxTPOUg2GJWjojVKCJZjEpJpamZoftBM1IrBwUiTSiSng41aWmlKpGOj0oQ25gknx7TQPCQiCYEcRHg/vPNeXrytFXuz91p77bV+v0//VtvtrU/P4uq6nue+e2zcuHFjAQBa2lZdvQAAoOspCAAABQEAoCAAAIqCAAAoCgIAoCgIAICiIAAAioIAACil9GzrD/bo0aOW62hZnbFRpGtTGx29Nq5LbbhnGpd7pjG19broEAAACgIAQEEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAKUdWxdDe5xzzjmRx48fH/moo46KfP3110eeOnVq5HXr1tV4dQC1cdZZZ0XO33FHHnlk5IULF9Z1TW2lQwAAKAgAgFJ6bGzjMUjNcArV8OHDIy9evDjyX//618hHHHFE5DfffLPma2rWk9vuvffeyGPGjImc15r/2Q8++ODIixYtqu3i2sjJbY2pWe+Zjrj66qsjX3jhhZGHDh0aecmSJTVfR6veM7vuumvkf/zjH5Hzv48Pf/jDkes9MnDaIQDQZgoCAKC13jI4/vjjI+cWyuuvvx557dq1dV1Ts3r66acj55FB9sorr0T+17/+VfM10X75nvnUpz4V+eSTT46cR26jRo2K7F6qra233jrySSedFLkzRiq0z8SJE7t6CZ1ChwAAUBAAAC02Mhg7dmzFz3//+99HXr16db2W09Ta8u/xmWeeifzyyy/Xcjlsxkc+8pHIZ555ZuQ8Gpg3b17kvOFKfkPk3HPPjTxz5sxOXmXz2XPPPSM///zz7fprt9rq3f8/t8cee0TOI5yVK1d2YHW01ZQpU7p6CZ1ChwAAUBAAAAoCAKC0wDME3/ve9yLnV6geffTRyFdddVVd19QKPvjBD3b1EtiMiy++OPJFF10UudorowsWLNjs78zPE1DZscceG/m2226L/Pjjj0f+xCc+EXn58uXt+v0vvvhi5LfeemsLVkhbnH/++ZF32223yPnZjvnz50du1AONMh0CAEBBAAC0wMggjwlyK+e+++6LvG7durquCbpKPgRn8uTJkb/1rW9F/u53vxt5zZo17fr9r732WgdW1xoOP/zwyH379o28++67R867EFZzzDHHdO7CaJcJEyZE7tOnT+QNGzZEvvHGG+u6po7SIQAAFAQAQJOODI444ojIO+64Y+R8iNHs2bPruiboKnm3wfyE++jRoyPnNwva4qCDDoo8bdq0yCeeeOIWrLD57bfffpHzbo7Z/fffH3nZsmWb/Z2DBw+u+Pn69evbuTraaqeddoo8cODAij+zZMmSyHkX3O5AhwAAUBAAAE06Mrjwwgsjb7fddpHzxhAvvfRSXdfEf3rjjTe6egktIbeozz777MjtHRNsu+22kfPT04cddljkvDFR3min1Y0aNSpytVb/I4880q7fmZ9yz/JmR3Suvfbaq2LO8p8z3e07TocAAFAQAABNNDLIbcuxY8dW/Jl77rmnXstpSSeddFLkQw45ZLM/v3r16loup6UdeuihkT/0oQ9Fbst5BO9///sjf/7zn4/80Y9+NHK+337wgx9ENiZ4Vx6xVPtOym8+PfDAAzVfE+3Xs+e7f0xecsklkXv06FHx5x988MGar6lWdAgAAAUBANBEI4N8lGtu1eU23Le//e26rqnV3H333ZH/8pe/RM5PWGd5w5zp06dHfvbZZ2uwutaS367J++Ln48Cfe+65yOPHj4+c3xTo3bt35I0bN0a+7rrrIud7j3flsyLyOC3Lx+MuWrSoU/6+d9xxR6f8Hv7XLrvsEvm4446LnO+HV199NXL+HuxudAgAAAUBANDNRwZ5X+mRI0dGzq2cK6+8sq5ramX5GOm1a9dGzk/j5iOot99++8jbbLNNjVfXWv7whz9Ezu39U089NXK+T/Le+UcffXTkZ555JnI+Ivn888/vvMU2qTy6rGbevHmR82jnnXfeqfjz+Yn3fHRylsdFy5cv3+wa+O/yWzfV3HLLLZG788hThwAAUBAAAN18ZJCf4h02bFjkfE7Bk08+Wdc1tbLcwhwwYEDk3JresGFDxc+pndzeb0urP79lcNppp0WeOnVqp66LUm6++ebIl156aeS8B/6qVasi/+lPf4p85JFHVvyd+Un4m266qVPW2cpOOOGErl5C3egQAAAKAgCgm48Mdt5554qfz549O3J3O36yO1u/fn3k/MYBjS+ffTBz5szI+SyDFStW1HVNrSC/3TF06NCKOR9Tnd+mqubtt9/upNW1rh122CHyBRdcEDmPOd96663I11xzTX0WVmM6BACAggAA6OYjg9zK+eMf/xh5xowZXbGclrdmzZrI+cloGlPecOXss8+OnO+rfN4B7fOrX/0qcj7bI/vd734XuS1v3QwcODDy0qVLK/7M7bff3sYVUs2UKVMi5+uS85w5cyLnswy6Mx0CAEBBAAB0w5FB3iSi2oY3QGUjRoyInDfFufPOOyM//vjj9VxS01q8eHHFTOPba6+9Nvszt912Wx1WUl86BACAggAA6CYjg+HDh0f+5je/WfFnmrF900pyi27RokVduJLmlkduc+fOjXzttdd2xXKgYey9996RJ06cWPFn8tsEzfJmQaZDAAAoCACABh4Z7LrrrpHvvvvuyPvtt1/Fz2+88cb6LIyaGDRoUFcvoan069cv8kUXXRR5//33jzxu3Li6rgka2XnnnRc5H9+e32C74YYbIr/wwgt1WVc96RAAAAoCAKCBRwZ5A5Xc5symTZsWefXq1TVfE7WTz6Jgy+QjW3/9619HzuMDY4LuLV9LOi7fM6NHj46cN7175ZVXIl955ZX1WVgX0SEAABQEAICCAAAoDfwMwRlnnBG52hnUjz76aF3XBI2md+/ekb/0pS9FPvLIIyN/7nOfi9yMr0q1kj333LPi5/kZqvx9yX+Xn0+r9qxasz83kOkQAAAKAgCgwUYGQ4cOjTxmzJiKP/ONb3yjXsuhA370ox9FrnYts9NPPz1y3lmP/27WrFmRJ02aFPm6666LfPPNN9dxRdTSY489VvHzVatWRTYy6Lg8mm6lXXB1CAAABQEA0GAjg2222SbywIEDI+enPJvxDOpm9NOf/rRipmMuv/zyTf7z5MmTI+cxwdSpU+u1JLpIPnTnhz/8YeT169d3xXK6pYULF0bu2bOh/jjsEjoEAICCAAAopcfGNj6S2qNHj1qvpSV1xhPBrk1tdPTa1OK6PPXUU5v853nz5kX+6le/2ul/v0bknmlcjXjP0PbrokMAACgIAAAjgy6n/dm4tD8bk3umcblnGpORAQDQZgoCAEBBAAAoCACAoiAAAEo73jIAAJqXDgEAoCAAABQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQFEQAABFQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUBQEAEBREAAARUEAABQFAQBQFAQAQCmlZ1t/sEePHrVcR8vauHFjh3+Ha1MbHb02rkttuGcal3umMbX1uugQAAAKAgBAQQAAFAUBAFAUBABAURAAAEVBAAAUBQEAUNqxMRFsqV69elX8fP369ZE7Y7MZgEay9dZbR37nnXe6cCVto0MAACgIAIAWHhn07PnuP3reP/vtt9/uiuU0nZNOOinyZZddFvm9731v5AULFkT+9Kc/HXnVqlU1Xh00r/zdlsdy1E7+M+Tqq6+OfPzxx0ceOXJk5BUrVtRnYe2kQwAAKAgAgBYbGfTt2zfyokWLIuf2zWGHHRbZ+KB9ttrq3fpy3LhxkUeMGBE5P3V71FFHRR40aFBkI4PGt80221T8fN26dXVeCaWUcvbZZ0c+77zzIk+YMCFy/s6jc+29996Rv/CFL0TO98m+++4b+ZFHHqnPwtpJhwAAUBAAAC02MjjrrLMi56fdlyxZEnnDhg11XVMzyZsLvfnmm5v9mWXLlkVeunRpzdZF++QnpgcPHhz5M5/5TOQzzzwz8po1ayJPnjw58hNPPLHJ77X5VOfK7eiLL7448i677BL5jTfeqOuaWkm+T77yla9E7tOnT+T850n+vmtUOgQAgIIAAGiBkcG2224b+ZRTTomc95WeMWNGxc/Zcv369av4eW4bv/TSS5HXrl1b8zVRXf/+/SNPmTIlch4T5E1unnrqqci9e/eOPGzYsMj//6l2m+T8p9z2r3a2R7VRS/5r88jt3//+d2Qjg9rJo4ETTjghcrWN7lauXFmfhXWADgEAoCAAABQEAEBpgWcIDjjggMgHHnhg5Jdffjny9ddfX9c1Nas8O9ttt90qfp53M8yHsHglrT569eoVOe9uN3HixMgvvvhi5FmzZkW+/fbbI+frmJ8bcE0376CDDor8s5/9LPI999wT+etf/3rkt956K3K+l7bbbrvI+TXfv/3tb5HtHFk7o0aNijx06NCKP/PYY49Ffv3112u+po7SIQAAFAQAQJOODPIBOt/5znci53ZmboXmndbYcrmNPGDAgMj5emQ77rhjZO3l2sn//n/84x9H/sAHPhB50qRJkfPrgm15VTDv9JmvKZXla7DPPvtEzt9DV1xxReR8X+W81157Vfw9f//73ztvsWwij2wuvPDCyPm65N0Jb7jhhsjd4ZV2HQIAQEEAADTpyGD//fePPHLkyMi5/ZlbOXSO3DbLbepq44DcajYyqJ3DDz888muvvRY5H/a1fPnydv3OvBNlPtjlrrvuipyfdm91eVe7PKrJ90l+Ij23pnPOO6/mt0SGDBkSOR/cRufK/7vPb63la7RixYrId955Z30W1kl0CAAABQEA0EQjg9x6mz59euTcyrn//vsj580+6Bz5LY7cIs3jgPyk7RNPPFGfhbWgfPDN+PHjI99yyy2Rc2uzmjwGes973hM530t585vZs2e3f7Et4MQTT4yc75N8qNfDDz8cOR+KU21jrzwmyN9/DzzwQMcXTEV5/DZo0KDI+Tsuj8ryQVPdgQ4BAKAgAACaaGSQ3yz4+Mc/Hjm3ci6//PKKn9M58phg++23j5z/Xef2Z2655c9dm4475JBDIp988smRn3nmmchPP/10xb82n/9x6qmnVvw9+drNmTMn8quvvrqFK25u+bsny5tA3XrrrZHzKCFvdLN69erIS5cujZzHP7/97W87tFY2lccxp59+esXP83dWPvOjLRt7NRIdAgBAQQAAdPORQX5ad+7cuZHzEa8vvPBCZE+111Y+jrV3796R85PR2WGHHRY5X0tHtnZc3jRl+PDhkS+55JLIY8aMiZzb0vlY1zwa6N+/f+S8GVE+L4R39e3bN3K+BrmNnI9eX7lyZeR8ParJbyLk3N2ebG90+XvtYx/7WOQ85szfWXlzru42/tQhAAAUBABANx8ZvO9974u87777Rs6b33zta1+LnJ/QpfPlFlpbPt9ll10i53b066+/3rkLa0Hz58+PnPdTz8dS59byP//5z8h5v/z8+UMPPRT5F7/4ReTu1hatlzx6yWPMak+eVztCN98/u+22W+Sjjz468g477BA5j9/ouGqbEWX57Z2cuxsdAgBAQQAAdPORwbhx4yLnp9oXL14c+Y477qjnklpabkHnlme1lnLe2CNfPzruxRdfjHzGGWdEzuOAPFrbcccdI+e3D/K+7AsWLIhsTLB5+Yn0PA7I44Orrroqch4H5LMiRowYETlvDrXzzjtX/PsOHDhwC1fM/8ljmgsuuCBytbehLr300oqfdzc6BACAggAA6IYjg9zKGTt2bOTcor7iiisir1q1qj4LY5MjpfMmK7mFWW2TotxGpXO98cYbkfO//zw+2H333SPnNxFeeumlyI4Mb58///nPkfObBfka5DM/TjvttMj5DYJ8rsFBBx1U8ffkn/GWTsfl6zJy5MiKP/PKK69Evu+++2q+pnrQIQAAFAQAQDccGeT286GHHho5t+TmzZtX1zXxv/J+6tU2Vslyy7PaKIHa2WeffSLn8Vt++yOPDLxZ0D75DacJEyZEXrNmTeRnn3028rJlyyLn8cxOO+0U+Zxzzomcz5PI996SJUs6smxKKZ/85CcjV9uM6De/+U3kPLLpznwLAwAKAgCgG44MvvzlL0fOx4vmJ3rzU9XUTx4N5Lc72jIyGDJkSOTnnnuuBqujlE03v8kbFuWNoa688srIy5cvr8/CmlAeY/7yl7/c4t+Tn2afNWtW5MmTJ0fOIwZntmyZ/H00bdq0ip/na3rTTTdFbpZxmg4BAKAgAAC6ycigX79+kc8999zIuU0zderUyPkJd+onX48+ffpU/DyPD/J1qnYkLB2XzymYPn165LwX/qRJkyLncxBoLHkUt2LFish5I518RgVtN3jw4MjDhg2r+DN5fPPkk0/WfE31pkMAACgIAIAGHhnkJzvzUaB5n/WHHnoo8oMPPliXdVFdHg1UGwHkn8mbqeTNWui4vJnK3LlzI++xxx6Rjz322Mgvv/xyfRZGh+T7Km9YVG0UR9tddNFFkfNbN/l76rOf/WzkZhxz6hAAAAoCAKCBRwbHHXdc5AMPPDDym2++GfmUU06J3CwbQzSL/NRzNXn/b0frdlx+Sjrvo7/nnntGzpsR5Sem6R7ymyE77LBD5HXr1nXFcrq9asdPZwsXLoz88MMP13xNXUmHAABQEAAACgIAoDTYMwQ9e767nHzASn4+4LLLLotsBtq48muE1Q43yodTHXDAAZEdbtR2+dWzm2++OfLBBx8c+Zprrok8f/78yJ676X569epVMednCFzXths7dmzkvKNnftUwH3TU7K9H6xAAAAoCAKDBRga5hbzPPvtEXrp0aeRbb721rmui7fIOaTNnzox87bXXRs7jg5UrV0bOh7ZQXT7oq5RS5syZE3n06NGR77333sgzZsyIbBe77i2/WpoPMcr3kpFB2+VXn1evXh15wYIFkfOOuM1OhwAAUBAAAA02MsgHGuUdofLT00uWLKnrmtgy3//+9yPfeOONkfv37x85n+een+plU/m++P+7qeUxwc9//vPIX/ziFyMbxzSP559/PnI+kCqP6IwM2u6uu+6KPGTIkMh5lNBKdAgAAAUBAFBKj41t7C9V21ymVvKhE/npz2Zrh3XGP0+9r02r6Oi16azrkjegOeaYYzb574YOHRr5Jz/5SeRmPuymle+ZvO4BAwZEzuO3/PZBvTXKPcOm2npddAgAAAUBANDAI4NW0crtz0bXKO3P/JZBHh+U0pp72LtnGlej3DNsysgAAGgzBQEA0FgbEwH/KZ8/0KobpgC1p0MAACgIAIB2vGUAADQvHQIAQEEAACgIAICiIAAAioIAACgKAgCgKAgAgKIgAACKggAAKKX8D0xarAAPz3yjAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# reconstructing training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(vqvae_model, train_imgs, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about images from the test set?"
      ],
      "metadata": {
        "id": "nuUZqnsPxbWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(vqvae_model, test_imgs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "2edfd86f-e81c-4228-a1f5-9d57013e6b30",
        "id": "voeZ3wVDxbWL"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is no overfitting at work here apparently. We can quantify this by computing the reconstruction loss over the test dataset (below) and compare it to the reconstruction loss over the training dataset at the end of training (check the training cell above).<br>\n",
        "\n",
        "Note that the training loss included a commitment term, which you do not need to include here as we want to assess the reconstruction performance. Anywhere around 0.07 or better is quite good for our simple VQ-VAE!"
      ],
      "metadata": {
        "id": "zFda9KgexbWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vqvae_model.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "n = 0\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Pass the data through the model\n",
        "    predict = vqvae_model(data)\n",
        "    x_hat = predict['x_rec']\n",
        "\n",
        "    # Compute the AE loss\n",
        "    loss = F.binary_cross_entropy_with_logits(x_hat, data, reduction = 'mean')\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7009034-a783-41a5-f1a4-34c058b40b81",
        "id": "_mt-7LN_xbWL"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:02<00:00, 69.68batch/s, loss=0.0657]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Generating images from the VQ-VAE the naive way!\n",
        "\n",
        "(or rather, failing at doing so!)<br>\n",
        "\n",
        "How do we use our VQ-VAE as a generative model? Unfortunately it does not come with a prior! (this is why, in the second part of the lab, we will build a prior on top of the tokenized representation) <br>\n",
        "\n",
        "We might have the naive idea that sampling uniformly at random the image tokens is good enough as a prior, and can be decoded into good-looking digits. Let's try it!"
      ],
      "metadata": {
        "id": "dCF8nLEcUaWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_naive(vq_vae, n_images=5):\n",
        "    output_tokens = torch.randint(high=2, size=(n_images, 7, 7))\n",
        "    x_hat = vqvae_model.decode(output_tokens)\n",
        "    return torch.sigmoid(x_hat)"
      ],
      "metadata": {
        "id": "l6rPY7mVVbTJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PWa87NaXVbTL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "7ef31ddd-0205-47c5-bcf2-57c97bf86f1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "imgs_generated = generate_images_naive(vqvae_model, n_images=5)\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This does not look too good :/ And indeed you will not make it work. We can scrap this idea and move to the second part of this lab."
      ],
      "metadata": {
        "id": "UfjFdVSgsbOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tokenized MNIST dataset/dataloader"
      ],
      "metadata": {
        "id": "HgvUhJPLIxdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MaskGIT-like model that we will build \\& train next processes tokenized representations. To avoid repeatedly recomputing these tokenized images from the original images on the fly in the training loop, let us build a dataset and dataloader for tokenized MNIST images, that directly stores once and for all and returns the tokenized representations. We will use this data loader in the MaskGIT training loop!"
      ],
      "metadata": {
        "id": "OLUF-ABis5TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tMNIST(object):\n",
        "    def __init__(self, vq_vae, mnist_loader, device):\n",
        "        vq_vae = vq_vae.to(device)\n",
        "        self.data = self.process(vq_vae, mnist_loader, device)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i]\n",
        "\n",
        "    def process(self, vq_vae, mnist_loader, device):\n",
        "        vq_vae.eval()\n",
        "        codes = []\n",
        "\n",
        "        with tqdm(mnist_loader, unit=\"batch\") as tepoch:\n",
        "            for data, labels in tepoch:\n",
        "                # Put the data on the correct device:\n",
        "                data = data.to(device)\n",
        "\n",
        "                # Pass the data through the model\n",
        "                encoding_indices, _, _ = vq_vae.encode(data)\n",
        "                codes.append(encoding_indices.detach().clone().cpu().numpy())\n",
        "\n",
        "        return np.concatenate(codes, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.data)[0]"
      ],
      "metadata": {
        "id": "nHQVyhJjEe0R"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the tokenized MNIST datasets\n",
        "tmnist_trainset = tMNIST(vqvae_model, mnist_train_loader, device)\n",
        "tmnist_testset = tMNIST(vqvae_model, mnist_test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcdgQaaIHg90",
        "outputId": "4755a2f5-8167-4ebd-aa16-5d26400fae7b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:07<00:00, 118.18batch/s]\n",
            "100%|██████████| 157/157 [00:01<00:00, 95.03batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create reduced size datasets if you want\n",
        "max_tmnist_size = 10000\n",
        "tmnist_trainset_reduced = torch.utils.data.random_split(tmnist_trainset, [max_tmnist_size, len(tmnist_trainset)-max_tmnist_size])[0]\n",
        "\n",
        "max_tmnist_size = 1000\n",
        "tmnist_testset_reduced = torch.utils.data.random_split(tmnist_testset, [max_tmnist_size, len(tmnist_testset)-max_tmnist_size])[0]"
      ],
      "metadata": {
        "id": "itfSmhTbH6WF"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256"
      ],
      "metadata": {
        "id": "1z4jhnyDuHab"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenized MNIST dataloaders\n",
        "tmnist_train_loader = torch.utils.data.DataLoader(tmnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "tmnist_test_loader = torch.utils.data.DataLoader(tmnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "2ZQX6IeIH6WG"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f423583-64fa-46ff-b20d-ce78849d004b",
        "id": "6bWgTlgUH6WG"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 7, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "tmnist_trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. AE-MaskGIT"
      ],
      "metadata": {
        "id": "kRrSOIFYEZo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to build our prior on top of the tokenized representations, as a modified __MaskGIT__ model. MaskGIT stands for 'Masked Generative Image Transformer' and only has generative capabilities. Our pretext task for this lab is autoencoding (we want to build a fancy VAE!), hence we will modify the MaskGIT model in a very naive way to give it autoencoding capabilities on top of its generative capabilities (more about that below)."
      ],
      "metadata": {
        "id": "UiFMm7buulTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__How is a MaskGIT trained?__ A number of tokens from the input token grids are randomly masked (replaced by a [msk] token), and the transformer model is trained to predict all tokens from what it can see. Various amounts of masking are used during training, from masking everything to masking only a small portion. In our case, there are up to 7x7=49 tokens per MNIST sample that can be masked or not."
      ],
      "metadata": {
        "id": "o24BxiZdvkRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__How does MaskGIT generate images?__ We generate an image with valid tokens by iteratively predicting them from an image with some tokens set to the [msk] token, across multiple timesteps. We start from all tokens set to [msk] at timestep 0. Various schemes can be used, we can adjust : the amounts of tokens predicted at each time step, various strategies for which tokens to predict (highest confidence, or random choice, or ...), various strategies for which token to set it to (the most likely, or randomized according to the predicted probabilities, or...), etc."
      ],
      "metadata": {
        "id": "EyesqqO7wpYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__How will we give MaskGIT autoencoding capabilities?__ We will add a number of 'code tokens' with values [cde] depending on the MNIST sample. These code tokens will be concatenated to the image tokens and be encouraged to incorporate as much information about the sample as possible to make the reconstruction more accurate, during training. In addition, we will add a `code_encoder` and a `code_decoder` (simple MLPs) to map these code tokens to and from the final vector latent representation of the sample (say, a vector with dimension 10).\n",
        "\n",
        "With this model, encoding the tokenized images will use a `CodeTransformer` to predict the code tokens, then the `code_encoder` to obtain the latent vector. Decoding the tokenized image representation of a sample from its latent vector will first involve the `code_decoder` to obtain the code token values, then a `MaskTransformer` which performs conditional generation of the image tokens, conditioned on the code tokens."
      ],
      "metadata": {
        "id": "CJfmWbYMyTLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__How do we generate new images with this modified AE-MaskGIT?__ We generate a latent vector from the prior N(0,Id), and we decode (do conditional generation of) the image tokens based on this latent vector as above."
      ],
      "metadata": {
        "id": "ebAccZah0SH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Building AE-MaskGIT"
      ],
      "metadata": {
        "id": "GXnAgSh-1yK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first classes, `PreNorm`, `FeedForward`, `Attention` and `TransformerEncoder`, are building blocks for `CodeTransformer` and `MaskTransformer`. I give them to you already completed because you have done enough of this in the past sessions! Take a look at `Attention`, which this time uses the optimized implementation as mentioned in the course."
      ],
      "metadata": {
        "id": "PARkaUAd2aAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, fn):\n",
        "        \"\"\" PreNorm module to apply layer normalization before a given function\n",
        "            :param:\n",
        "                dim  -> int: Dimension of the input\n",
        "                fn   -> nn.Module: The function to apply after layer normalization\n",
        "            \"\"\"\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        \"\"\" Forward pass through the PreNorm module\n",
        "            :param:\n",
        "                x        -> torch.Tensor: Input tensor\n",
        "                **kwargs -> _ : Additional keyword arguments for the function\n",
        "            :return\n",
        "                torch.Tensor: Output of the function applied after layer normalization\n",
        "        \"\"\"\n",
        "        return self.fn(self.norm(x), **kwargs)"
      ],
      "metadata": {
        "id": "OFxfWS5kV0ee"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        \"\"\" Initialize the Multi-Layer Perceptron (MLP).\n",
        "            :param:\n",
        "                dim        -> int : Dimension of the input\n",
        "                dim        -> int : Dimension of the hidden layer\n",
        "                dim        -> float : Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim, bias=True),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim, bias=True),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass through the MLP module.\n",
        "            :param:\n",
        "                x -> torch.Tensor: Input tensor\n",
        "            :return\n",
        "                torch.Tensor: Output of the function applied after layer\n",
        "        \"\"\"\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "IcZ4yQjDXt6G"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.):\n",
        "        \"\"\" Initialize the Attention module.\n",
        "            :param:\n",
        "                embed_dim     -> int : Dimension of the embedding\n",
        "                num_heads     -> int : Number of heads\n",
        "                dropout       -> float : Dropout rate\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.dim = embed_dim\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads=num_heads, dropout=dropout, batch_first=True, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass through the Attention module.\n",
        "            :param:\n",
        "                x -> torch.Tensor: Input tensor\n",
        "            :return\n",
        "                attention_value  -> torch.Tensor: Output the value of the attention\n",
        "                attention_weight -> torch.Tensor: Output the weight of the attention\n",
        "        \"\"\"\n",
        "        attention_value, attention_weight = self.mha(x, x, x)\n",
        "        return attention_value, attention_weight"
      ],
      "metadata": {
        "id": "DCCVbNw0Xyfl"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, dim, depth, heads, mlp_dim, mlp_dropout=0., attn_dropout=0.):\n",
        "        \"\"\" Initialize the Attention module.\n",
        "            :param:\n",
        "                dim       -> int : number of hidden dimension of attention\n",
        "                depth     -> int : number of layer for the transformer\n",
        "                heads     -> int : Number of heads\n",
        "                mlp_dim   -> int : number of hidden dimension for mlp\n",
        "                dropout   -> float : Dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, Attention(dim, heads, dropout=attn_dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=mlp_dropout))\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\" Forward pass through the Attention module.\n",
        "            :param:\n",
        "                x -> torch.Tensor: Input tensor\n",
        "            :return\n",
        "                x -> torch.Tensor: Output of the Transformer\n",
        "                l_attn -> list(torch.Tensor): list of the attention\n",
        "        \"\"\"\n",
        "        l_attn = []\n",
        "        for attn, ff in self.layers:\n",
        "            attention_value, attention_weight = attn(x)\n",
        "            x = attention_value + x\n",
        "            x = ff(x) + x\n",
        "            l_attn.append(attention_weight)\n",
        "        return x, l_attn"
      ],
      "metadata": {
        "id": "QvtxwWxlYmGO"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `MaskTransformer` class is one of the key ingredients of MaskGIT, yet is quite succinct. It is responsible for predicting image tokens given the image tokens themselves (some of them masked), and the code tokens. The masking is done outside of this class, in `AEMaskGIT`."
      ],
      "metadata": {
        "id": "jPAgR_Cx3HKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskTransformer(nn.Module):\n",
        "    def __init__(self, n_patches, n_code_tokens=1, embedding_dim=128, codebook_size=65, depth=3, heads=4, mlp_dim=256, mlp_dropout=0.1, attn_dropout=0.):\n",
        "        \"\"\" Initialize the Transformer model.\n",
        "            :param:\n",
        "                n_patches      -> int:     Number of input/output visual patch tokens\n",
        "                n_code_tokens  -> int:     Number of additional code tokens (default: 1)\n",
        "                embedding_dim  -> int:     Hidden dimension for the transformer (default: 128)\n",
        "                codebook_size  -> int:     Size of the codebook + 1 mask token (default: 65)\n",
        "                depth          -> int:     Depth of the transformer (default: 3)\n",
        "                heads          -> int:     Number of attention heads (default: 4)\n",
        "                mlp_dim        -> int:     MLP dimension (default: 256)\n",
        "                dropout        -> float:   Dropout rate (default: 0.1/0.)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_code_tokens = n_code_tokens\n",
        "        self.n_patches = n_patches\n",
        "        self.codebook_size = codebook_size\n",
        "\n",
        "        # We need to map the token to token embeddings. Create a nn.Embedding object,\n",
        "        # with size the size of the codebook and the correct embedding dimension\n",
        "        self.tok_emb = nn.Embedding(codebook_size, embedding_dim)\n",
        "\n",
        "        # These positional embeddings are added to the embeddings (image+code)\n",
        "        # before passing them through the transformer\n",
        "        self.pos_emb = nn.init.trunc_normal_(nn.Parameter(torch.zeros(1, self.n_patches+self.n_code_tokens, embedding_dim)), 0., 0.02)\n",
        "\n",
        "        # Class TransformerEncoder(), with all params correctly set\n",
        "        self.transformer = TransformerEncoder(embedding_dim, depth, heads, mlp_dim, mlp_dropout, attn_dropout)\n",
        "\n",
        "        # Last layer after the Transformer block, to predict token logits\n",
        "        # For implementation convenience, we actually predict a logit for the\n",
        "        # [msk] token value, that we discard later on.\n",
        "        self.to_logits = nn.Linear(embedding_dim, self.codebook_size, bias=True)\n",
        "\n",
        "    def forward(self, img_token, code_token, return_attn=False):\n",
        "        \"\"\" Forward.\n",
        "            :param:\n",
        "                img_token      -> torch.LongTensor: bsize x 7 x 7, the encoded image tokens\n",
        "                code_token     -> torch.FloatTensor: bsize x n_code_tokens x hidden_dim, the code tokens\n",
        "                return_attn    -> Bool: return the attn for visualization\n",
        "            :return:\n",
        "                logit:         -> torch.FloatTensor: bsize x 7 x 7 x 65, the predicted logit\n",
        "                attn:          -> list(torch.FloatTensor): list of attention for visualization\n",
        "        \"\"\"\n",
        "        b, w, h = img_token.size()\n",
        "\n",
        "        # Compute token embeddings from integer tokens, and rearrange from (b, w, h, d)\n",
        "        # to (b, w*h, d)\n",
        "        img_tok_embeddings = rearrange(self.tok_emb(img_token), 'b w h d -> b (w h) d')\n",
        "\n",
        "        # Concatenate code tokens at the end of the image token embeddings\n",
        "        tok_embeddings = torch.cat([img_tok_embeddings, code_token], -2)\n",
        "\n",
        "        # Add position embeddings\n",
        "        x = tok_embeddings + self.pos_emb\n",
        "\n",
        "        # Pass the embeddings through the transformer\n",
        "        x, attn = self.transformer(x)\n",
        "\n",
        "        # Extract the transformed image token embeddings, and compute output logits\n",
        "        # for these tokens\n",
        "        logit = self.to_logits(x[:, :self.n_patches])\n",
        "\n",
        "        # Rearrange the logits from (b, w*h, d) to (b, w, h, d)\n",
        "        logit = rearrange(logit, 'b (w h) d -> b w h d', w=w, h=h)\n",
        "\n",
        "        if return_attn:  # return list of attention\n",
        "            return logit, attn\n",
        "\n",
        "        return logit"
      ],
      "metadata": {
        "id": "TGMzfP7cbM2G"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next class is `CodeTransformer`. It is responsible for predicting a fixed number of code tokens from the input image tokens, for encoding purposes. __These code tokens are incorporated in the same manner as the [cls] token in ViT classification models__ i.e., they are concatenated at the end of the image token sequence and processed jointly by the transformer blocks. The code tokens are initialized via a learnable embedding (just like class tokens)."
      ],
      "metadata": {
        "id": "baB33wIc5jOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CodeTransformer(nn.Module):\n",
        "    def __init__(self, n_patches, n_code_tokens=1, code_dim=128, embedding_dim=128, codebook_size=64, depth=3, heads=4, mlp_dim=256, mlp_dropout=0.1, attn_dropout=0.):\n",
        "        \"\"\" Initialize the Transformer model.\n",
        "            :param:\n",
        "                n_patches      -> int:     Number of input/output visual patch tokens\n",
        "                n_code_tokens  -> int:     Number of additional code tokens (default: 1)\n",
        "                code_dim       -> int:     Dimension of the output codes (default: 128)\n",
        "                embedding_dim  -> int:     Dimension of the hidden embeddings (default: 128)\n",
        "                codebook_size  -> int:     Size of the codebook (default: 64)\n",
        "                depth          -> int:     Depth of the transformer (default: 3)\n",
        "                heads          -> int:     Number of attention heads (default: 4)\n",
        "                mlp_dim        -> int:     MLP dimension (default: 256)\n",
        "                dropout        -> float:   Dropout rate (default: 0.1/0.)\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_code_tokens = n_code_tokens\n",
        "        self.code_dim = code_dim\n",
        "        self.n_patches = n_patches\n",
        "        self.codebook_size = codebook_size\n",
        "\n",
        "        # The token embeddings. Each embedding is embedding_dim dimensional.\n",
        "        # There is one embedding per token in the codebook, plus one embedding per\n",
        "        # additional code token\n",
        "        self.tok_emb = nn.Embedding(codebook_size + n_code_tokens, embedding_dim)\n",
        "\n",
        "\n",
        "        # Positional embeddings to add to the image and code token embeddings.\n",
        "        self.pos_emb = nn.init.trunc_normal_(nn.Parameter(torch.zeros(1, self.n_patches+self.n_code_tokens, embedding_dim)), 0., 0.02)\n",
        "\n",
        "        # Class TransformerEncoder(), with all params correctly set\n",
        "        self.transformer = TransformerEncoder(dim=embedding_dim, depth=depth, heads=heads, mlp_dim=mlp_dim, mlp_dropout=mlp_dropout, attn_dropout=attn_dropout)\n",
        "\n",
        "        # Last layer after the Transformer block\n",
        "        # to cast the code tokens to the correct output dimension (code_dim)\n",
        "        self.linear_head = nn.Linear(embedding_dim, self.code_dim, bias=True)\n",
        "\n",
        "    def forward(self, img_token, return_attn=False):\n",
        "        \"\"\" Forward.\n",
        "            :param:\n",
        "                img_token      -> torch.LongTensor: bsize x 7 x 7, the encoded image tokens\n",
        "                return_attn    -> Bool: return the attn for visualization\n",
        "            :return:\n",
        "                codes:         -> torch.FloatTensor: bsize x n_code_tokens x code_dim, the predicted codes for the samples\n",
        "                attn:          -> list(torch.FloatTensor): list of attention for visualization\n",
        "        \"\"\"\n",
        "        b, w, h = img_token.size()\n",
        "\n",
        "        # Indices of the additional code tokens in the codebook\n",
        "        code_token = torch.arange(self.n_code_tokens, device=img_token.device) + self.codebook_size\n",
        "\n",
        "        # Repeat across minibatch dimension\n",
        "        code_token = repeat(code_token, 'n -> b n', b=b)\n",
        "\n",
        "        # Concatenate code tokens to the image tokens (after the image tokens)\n",
        "        input = torch.cat([rearrange(img_token, 'b w h -> b (w h)'), code_token], -1)\n",
        "\n",
        "        # Convert from integer tokens to token embeddings\n",
        "        tok_embeddings = self.tok_emb(input)\n",
        "\n",
        "        # Add position embedding\n",
        "        x = tok_embeddings + self.pos_emb\n",
        "\n",
        "        # Transformer forward pass\n",
        "        x, attn = self.transformer(x)\n",
        "\n",
        "        # Compute the final output (discard image tokens, only pass the code tokens through the linear head)\n",
        "        codes = self.linear_head(x[:, -self.n_code_tokens:])\n",
        "\n",
        "        # Return\n",
        "        if return_attn:  # return list of attention\n",
        "            return codes, attn\n",
        "\n",
        "        return codes"
      ],
      "metadata": {
        "id": "unSQc4XHh0zB"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last class is the main wrapper class, `AEMaskGIT`, which implements most of the high-level routines for training, auto-encoding, encoding, decoding, and tokenized image generation. Fill the missing parts."
      ],
      "metadata": {
        "id": "NhLSHI-fjSpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AEMaskGIT(nn.Module):\n",
        "    def __init__(self, mask_transformer, code_transformer, latent_dim=10, n_code_tokens=1, code_dim=128, codebook_size=64, mlp_dim=128, decoding_steps=8):\n",
        "        super().__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_code_tokens = n_code_tokens\n",
        "        self.code_dim = code_dim\n",
        "        self.codebook_size = codebook_size\n",
        "        self.steps = decoding_steps\n",
        "\n",
        "        # MaskTransformer and CodeTransformer instances\n",
        "        self.mask_transformer = mask_transformer # useful to predict the masked tokens\n",
        "        self.code_transformer = code_transformer # useful to \"encode\" a tokenized image into a set of code tokens\n",
        "            # The code tokens will further be processed by self.code_encoder to\n",
        "            # transform them into a latent vector (or rather a Gaussian distribution of latent vectors, like in VAEs)\n",
        "\n",
        "        # To transform code tokens into variational posterior distributions of latent vectors\n",
        "        # q(z|x)\n",
        "        self.code_encoder = nn.Sequential(\n",
        "            nn.Linear(code_dim*n_code_tokens, mlp_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(mlp_dim, latent_dim*2)\n",
        "        )\n",
        "\n",
        "        # To go from a latent vector back to code tokens\n",
        "        self.code_decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, mlp_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(mlp_dim, code_dim*n_code_tokens)\n",
        "        )\n",
        "\n",
        "    def schedule(self, r, mode=\"cosine\"):\n",
        "        '''\n",
        "        Controls the masking ratio. During training r is uniformly sampled.\n",
        "        During testing r = t/T, with T the number of steps.\n",
        "        We pass r through a potentially nonlinear function to obtain the masking\n",
        "        ratio.\n",
        "\n",
        "        :param\n",
        "            r   -> torch.FloatTensor(): *, the input ratio\n",
        "        :return\n",
        "            mask_ratio -> torch.FloatTensor(): *, the ratio of tokens to mask\n",
        "        '''\n",
        "\n",
        "        if mode==\"linear\":\n",
        "            mask_ratio = 1-r\n",
        "        elif \"pow\" in mode:\n",
        "            exponent = float(mode.replace(\"pow\", \"\"))\n",
        "            mask_ratio = 1. - r**exponent\n",
        "        elif mode==\"cosine\":\n",
        "            mask_ratio = torch.cos(r * np.pi * 0.5)\n",
        "        elif mode==\"arccos\":\n",
        "            mask_ratio = torch.arccos(r) / (np.pi * 0.5)\n",
        "\n",
        "        return mask_ratio\n",
        "\n",
        "    def get_masked_tokens(self, img_tokens, mode=\"cosine\"):\n",
        "        '''\n",
        "        Replace the image tokens by *mask_token_id* according the the *mode* scheduler\n",
        "\n",
        "           :param\n",
        "              img_tokens    -> torch.LongTensor(): bsize * 7 * 7, the unmasked image tokens\n",
        "           :return\n",
        "              masked_tokens -> torch.LongTensor(): bsize * 7 * 7, the masked version of the tokens\n",
        "              mask          -> torch.LongTensor(): bsize * 7 * 7, the binary mask of the mask\n",
        "        '''\n",
        "\n",
        "        # Uniformly sample the \"time step\" as a continuous value between 0 and 1\n",
        "        # for each sample (tokenized image) in the minibatch\n",
        "        r = torch.rand(img_tokens.shape[0], device=img_tokens.device)\n",
        "\n",
        "        # Transform the time step into a masking ratio\n",
        "        # Hint: self.schedule\n",
        "        mask_ratio = self.schedule(r, mode)\n",
        "\n",
        "        # Initialize the output partially masked tokens by cloning the input tokens\n",
        "        masked_tokens = img_tokens.detach().clone()\n",
        "\n",
        "        # Sample the amount of tokens + localizations to mask\n",
        "        mask = torch.rand(size=img_tokens.size(), device = img_tokens.device) < mask_ratio.view(img_tokens.size(0), 1, 1)\n",
        "\n",
        "        # Replace the tokens to mask with the [msk] token (= self.codebook_size)\n",
        "        mask_token_id = self.codebook_size\n",
        "        masked_tokens[mask] = torch.full_like(masked_tokens[mask], mask_token_id)\n",
        "\n",
        "        return masked_tokens, mask\n",
        "\n",
        "    def encode(self, x):\n",
        "        '''\n",
        "        Maps from tokenized images to latent vector posterior distribution\n",
        "        q(z|x)=N(mu,sigma).\n",
        "\n",
        "        :args:\n",
        "            x -> LongTensor(), b x 7 x 7: the tokenized images\n",
        "        :return:\n",
        "            mu -> FloatTensor(), b x latent_dim: the means of the q(z|x)'s\n",
        "            lsigma -> FloatTensor(), b x latent_dim: the log st.d. of the q(z|x)'s\n",
        "        '''\n",
        "\n",
        "        # Compute the code tokens from the image tokens\n",
        "        # Hint: self.code_transformer\n",
        "        codes = self.code_transformer(x)\n",
        "\n",
        "        # Map from code tokens to latent mu and lsigma\n",
        "        z = rearrange(codes, 'b c d -> b (c d)')\n",
        "        z = self.code_encoder(z)\n",
        "\n",
        "        z = rearrange(z, 'b (d t) -> b d t', t=2)\n",
        "        mu = z[:,:,0]\n",
        "        lsigma = z[:,:,1]\n",
        "\n",
        "        return mu, lsigma\n",
        "\n",
        "    def reparameterize(self, mu, lsigma, deterministic=False):\n",
        "        \"\"\"\n",
        "        Samples from a normal distribution using the reparameterization trick.\n",
        "\n",
        "        :param\n",
        "            mu            -> torch.FloatTensor(): b x latent_dim, mean of the normal distribution.\n",
        "            lsigma        -> torch.FloatTensor(): b x latent_dim, diagonal st.d. of the normal distribution\n",
        "            deterministic -> bool, see return doc\n",
        "        :return, either a sample from qzx (deterministic=False), or just the\n",
        "            mean of qzx (deterministic=True). The former is useful at training\n",
        "            time. The latter is useful at inference time as the mean is usually\n",
        "            used for reconstruction, rather than a sample.\n",
        "        \"\"\"\n",
        "        if deterministic is False:\n",
        "            # Implements the reparametrization trick:\n",
        "            std = torch.exp(lsigma)\n",
        "            eps = torch.randn_like(std)\n",
        "            return mu + std * eps\n",
        "        else:\n",
        "            return mu\n",
        "\n",
        "    def decode_train(self, z, img_tokens):\n",
        "        '''\n",
        "        This is the main decoding routine at training time.\n",
        "        The input should be partially masked tokenized images and their latent codes.\n",
        "        The output is the predicted logits for all (masked and non-masked) tokens.\n",
        "\n",
        "        :args\n",
        "            z -> FloatTensor(), b x latent_dim: the latent encodings for all tokenized images\n",
        "            img_tokens -> LongTensor(), b x 7 x 7: the (partially masked) tokenized images\n",
        "        '''\n",
        "\n",
        "        # Map from latent vectors z to code tokens\n",
        "        # Hint: self.code_decoder\n",
        "        # The output should be of shape (b, self.n_code_tokens, d)\n",
        "        z = self.code_decoder(z)\n",
        "        code_tokens = rearrange(z, 'b (c d) -> b c d', c=self.n_code_tokens)\n",
        "\n",
        "        # Predict the output logits from input image tokens and code tokens\n",
        "        # Hint: self.mask_transformer\n",
        "        logits = self.mask_transformer(img_tokens, code_tokens)\n",
        "\n",
        "        # Output\n",
        "        return logits[:,:,:,:self.codebook_size] # drop the last logit (of the mask token)\n",
        "\n",
        "    def forward(self, x, mode=\"cosine\", deterministic=False):\n",
        "        \"\"\"\n",
        "        This is the main call at training time. It attempts to encode input tokenized\n",
        "        images into a latent vector, then reconstruct the input tokens from partially\n",
        "        masked tokens and from the latent vector.\n",
        "\n",
        "        From the outputs of this routine, the reconstruction loss in token space as\n",
        "        well as the KL divergence penalty can be computed (see below section 4.2)\n",
        "\n",
        "        :param\n",
        "            x       -> torch.LongTensor(): b x 7 x 7, the image tokens\n",
        "        :return\n",
        "            logits  -> torch.FloatTensor(): b x 7 x 7 x codebook_size, the logits for each code indice at each location\n",
        "            mu      -> torch.FloatTensor(): b x latent_dim, the latent code mean for each image\n",
        "            lsigma  -> torch.FloatTensor(): b x latent_dim, the latent code std for each image\n",
        "            mask    -> torch.BoolTensor(): b x 7 x 7, the mask telling which tokens are masked\n",
        "        \"\"\"\n",
        "\n",
        "        # Encode x\n",
        "        mu, lsigma = self.encode(x)\n",
        "\n",
        "        # Sample from q(z|x)=N(z;mu,sigma)\n",
        "        # Hint: self.reparameterize\n",
        "        z = self.reparameterize(mu, lsigma)\n",
        "\n",
        "        # Mask some of the tokens in x\n",
        "        # Hint: self.get_masked_tokens\n",
        "        masked_tokens, mask = self.get_masked_tokens(x, mode)\n",
        "\n",
        "        # Predict logits of the masked (and non-masked) tokens\n",
        "        # Hint: self.decode_train\n",
        "        logits = self.decode_train(z, masked_tokens)\n",
        "\n",
        "        # Output\n",
        "        return logits, mu, lsigma, mask\n",
        "\n",
        "    def decode_test(self, z, mode=\"cosine\", temperature=1., randomize=\"warm_up\", deterministic=False):\n",
        "        '''\n",
        "        This is the main decoding routine at test/inference time.\n",
        "        It decodes a tokenized image from a latent vector z.\n",
        "\n",
        "        Starting from all masked tokens, it iteratively attempts to predict the\n",
        "        masked tokens in several iterations. At each iteration, only a subset of the\n",
        "        predicted tokens is kept, typically the ones with the highest confidence\n",
        "        if randomized=\"none\" or deterministic=True.\n",
        "        The tokens which are kept can alternatively be chosen randomly\n",
        "        (deterministic = False, randomize=\"warm_up\" or \"random\")\n",
        "\n",
        "        The number of predicted tokens retained at each iteration is controlled by\n",
        "        the time step, by self.schedule and by the mode argument.\n",
        "\n",
        "        The value for the predicted tokens is obtained from the predicted logits,\n",
        "        either as the logit argmax (deterministic = True), or by sampling from\n",
        "        the probability distribution defined by the logits (independently at each\n",
        "        location).\n",
        "\n",
        "        :args:\n",
        "            z -> FloatTensor(), b x latent_dim: the latent vectors\n",
        "        :return:\n",
        "            img_tokens -> LongTensor(), b x 7 x 7: the decoded tokenized images\n",
        "        '''\n",
        "\n",
        "        # Decode latent vectors into a set of code tokens\n",
        "        z = self.code_decoder(z)\n",
        "        code_tokens = rearrange(z, 'b (c d) -> b c d', c=self.n_code_tokens)\n",
        "\n",
        "        # Useful values\n",
        "        b = z.shape[0]\n",
        "        n_patches = self.code_transformer.n_patches\n",
        "        w = int(np.sqrt(n_patches))\n",
        "        mask_token_id = self.codebook_size\n",
        "\n",
        "        # Initialize to all masked tokens\n",
        "        img_tokens = torch.full((b, w, w), mask_token_id, device=z.device)\n",
        "        mask = torch.ones((b, w, w), device=z.device)\n",
        "\n",
        "        for t in np.arange(self.steps):\n",
        "            # Convert integer time step to continuous value in (0,1]\n",
        "            r = (t+1) / self.steps # the first step is already done with all masked, so we don't start at r=0\n",
        "\n",
        "            # Compute the masking ratio and the number of tokens to keep masked\n",
        "            mask_ratio = self.schedule(torch.Tensor([r]), mode)\n",
        "            mask_len = np.maximum(0,np.floor(mask_ratio.item()*n_patches).astype(int))\n",
        "\n",
        "            # Predict logits for all tokens, from the current code and image tokens\n",
        "            logits = self.mask_transformer(img_tokens.clone(), code_tokens)\n",
        "            logits = logits[:,:,:,:self.codebook_size] # drop the last logit (of the mask token)\n",
        "\n",
        "            if deterministic is True:\n",
        "                # Predict the token with maximum probability\n",
        "                conf, pred_tokens = torch.max(logits, dim=-1)\n",
        "            else:\n",
        "                # Sample the token from the softmax prediction\n",
        "                prob = F.softmax(logits * temperature, dim=-1)\n",
        "                distri = torch.distributions.Categorical(probs=prob)\n",
        "                pred_tokens = distri.sample()\n",
        "\n",
        "                # The confidence is either the probability of the chosen token,\n",
        "                # or a random value if randomized\n",
        "                conf = torch.gather(prob, 3, pred_tokens.unsqueeze(-1))\n",
        "                if randomize == \"warm_up\":  # choose random sample for the 2 first steps`\n",
        "                    conf = torch.rand_like(conf) if t < 2 else conf\n",
        "                elif randomize == \"random\": # choose random prediction at each step\n",
        "                    conf = torch.rand_like(conf)\n",
        "                elif randomize == \"none\":   # no randomization\n",
        "                    conf = conf\n",
        "                conf = conf.squeeze(-1)\n",
        "\n",
        "            # Mask the mask_len lowest confidence tokens\n",
        "            #   a. Compute the cut off under which to mask\n",
        "            sorted_confidence, _ = torch.sort(rearrange(conf, 'b w h -> b (w h)'), dim=-1)\n",
        "            indices = repeat(torch.Tensor([mask_len]).long().to(device), '1 -> b 1', b=b)\n",
        "            cut_off = torch.take_along_dim(sorted_confidence, indices, dim=-1) # Obtains cut off threshold given the mask lengths.\n",
        "            #   b. Mask below the cut off, i.e. tokens with lower confidence\n",
        "            mask = (conf <= cut_off.view(b, 1, 1))\n",
        "            #   c. Compute the output with correctly masked tokens\n",
        "            img_tokens = pred_tokens # initialize to predicted tokens\n",
        "            if t != (self.steps-1):\n",
        "                img_tokens[mask] = mask_token_id # mask tokens to mask\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def sample_pz(self, N, r=None):\n",
        "        '''\n",
        "        Samples from p_NCP(z)=N(z; 0,1)r(z) if r is provided, otherwise from\n",
        "        p(z)=N(z;0,1).\n",
        "\n",
        "        This routine is used to generate samples (see generate_samples).\n",
        "        '''\n",
        "\n",
        "        if r is None:\n",
        "            samples_pz = torch.randn(N, self.latent_dim, device=self.code_encoder[0].weight.device)\n",
        "        else:\n",
        "            samples_pz = self.sir(N, r)\n",
        "\n",
        "        return samples_pz\n",
        "\n",
        "    def sir(self, N, r, n_candidates=100):\n",
        "        '''\n",
        "        Sampling-Importance-Resampling routine to sample from p_NCP(z) \\propto p(z)r(z)\n",
        "        Complete this when you reach part 5 (detailed explanation provided there).\n",
        "\n",
        "        :args:\n",
        "            N -> int: the number of samples to generate\n",
        "            r -> instance of class r: the correction factor in p_NCP(z)\n",
        "            n_candidates -> the number of candidate samples from which to obtain a single sample\n",
        "\n",
        "        :return:\n",
        "            z -> FloatTensor(), shape (N, self.latent_dim): the samples from p_NCP(z)\n",
        "        '''\n",
        "\n",
        "        # Sample from N(0,1) a tensor of shape (N, n_candidates, self.latent_dim)\n",
        "        # hint: you can set the device to self.code_encoder[0].weight.device\n",
        "        z = torch.randn(N, n_candidates, self.latent_dim, device = device)\n",
        "\n",
        "        # Compute the weights r(z)\n",
        "        r_z = r(z)\n",
        "\n",
        "        # Sample 1 index (between 0 and n_candidates) per row with probability\n",
        "        # proportional to the row r_z\n",
        "        indices = torch.multinomial(r_z, num_samples=1)\n",
        "\n",
        "        # Extract, for each of the N rows, the vector corresponding to the sampled candidate index\n",
        "        # Hint: torch.take_along_dim\n",
        "        # Don't forget to squeeze the dimension of size 1 at the end to return a tensor of shape (N, self.latent_dim)\n",
        "        z = torch.take_along_dim(z, indices.unsqueeze(-1), dim=1).squeeze(dim=1)\n",
        "\n",
        "        # Return the samples from p_NCP\n",
        "        return z\n",
        "\n",
        "    def generate_samples(self, samples_pz=None, N=None, mode=\"cosine\", temperature=1., randomize=\"warm_up\", deterministic=False, r=None):\n",
        "        '''\n",
        "        This is the main routine to generate a sample (a tokenized image) from the\n",
        "        MaskGIT model.\n",
        "        '''\n",
        "        if samples_pz is None:\n",
        "            if N is None:\n",
        "                return ValueError(\"samples_pz and N cannot be set to None at the same time. Specify one of the two.\")\n",
        "\n",
        "            # If samples z are not provided, we sample N samples either\n",
        "            # - from the prior p(z)=N(0,Id), using sample_pz\n",
        "            # - from the NCP prior p_NCP(z)=p(z)r(z), using sample_pz (see part 5 of the lab)\n",
        "            samples_pz = self.sample_pz(N, r)\n",
        "\n",
        "        # Decode the z's to obtain samples in token space\n",
        "        generations = self.decode_test(samples_pz)\n",
        "        return generations\n"
      ],
      "metadata": {
        "id": "iFirZXdJqhJA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Training Losses"
      ],
      "metadata": {
        "id": "Kz3e9M6vcXVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In terms of losses, our AE-MaskGIT will be quite similar to a standard VAE model with:\n",
        "* a reconstruction loss: note that AE-MaskGIT effectively does multiclass classification (per token), as it is trained to predict the correct token; so the loss will be based on the cross entropy\n",
        "* a KL divergence penalty between $q(z|x)=N(z;\\mu,\\sigma)$ and $p(z)=N(z;0,1)$ as in a standard VAE\n",
        "\n",
        "We wrap these two losses within a `BetaVAELoss` class."
      ],
      "metadata": {
        "id": "l1Q8AIV-cez6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reconstruction_loss(reconstructions, data):\n",
        "    \"\"\"\n",
        "    Calculates the reconstruction loss for a batch of data. I.e. negative\n",
        "    log likelihood.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : torch.Tensor\n",
        "        Input data (batch of image tokens). Shape : (B, H, W).\n",
        "\n",
        "    reconstructions : torch.Tensor\n",
        "        Reconstructed data (logits). Shape : (B, H, W, D).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : torch.Tensor\n",
        "        Cross entropy, averaged over images in the batch but summed over\n",
        "        pixels.\n",
        "    \"\"\"\n",
        "    b, h, w, d = reconstructions.size()\n",
        "    logits = rearrange(reconstructions, 'b h w d -> b d h w')\n",
        "\n",
        "    # The pixel-wise loss is the cross-entropy, computed from\n",
        "    # reconstructions and data. It is summed over pixels and averaged across\n",
        "    # samples in the batch.\n",
        "    loss = F.cross_entropy(logits, data, reduction='sum')\n",
        "    loss = loss / b\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "CWrQbhOnbv7y"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL divergence term is computed as per the regularization term in slide 45 i.e., for each data sample in the mini-batch:\n",
        "$$\\frac{1}{2}\\sum_{j=1}^D (\\mu_j^2 + \\sigma_j^2 - 1 - \\log{\\sigma_j^2})$$"
      ],
      "metadata": {
        "id": "yIOMjQ7OyR9I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "pv2DR7M0yR9J"
      },
      "outputs": [],
      "source": [
        "def kl_normal_loss(mu, lsigma):\n",
        "    \"\"\"\n",
        "    Calculates the KL divergence between a normal distribution\n",
        "    with diagonal covariance and a unit normal distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mu : torch.Tensor\n",
        "        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
        "        D is dimension of distribution.\n",
        "\n",
        "    lsigma : torch.Tensor\n",
        "        Diagonal log st.d. of the normal distribution. Shape (batch_size,\n",
        "        latent_dim)\n",
        "    \"\"\"\n",
        "    # To be consistent with the reconstruction loss, we take the mean over the\n",
        "    # minibatch (i.e., compute for each sample in the minibatch according to\n",
        "    # the equation above, then take the mean).\n",
        "    lvar = lsigma*2\n",
        "    latent_kl = 1/2 * torch.sum(mu**2 + torch.exp(lvar) - 1 - lvar)\n",
        "\n",
        "    return latent_kl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `BetaVAELoss` puts it all together."
      ],
      "metadata": {
        "id": "186BASpoyR9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Cjm___fDyR9L"
      },
      "outputs": [],
      "source": [
        "class BetaVAELoss(object):\n",
        "    \"\"\"\n",
        "    Compute the Beta-VAE loss\n",
        "\n",
        "    :params\n",
        "        beta: (scalar) the weight assigned to the regularization term\n",
        "    :return\n",
        "        loss: the VAE autodifferentiable loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta):\n",
        "        self.beta = beta\n",
        "\n",
        "    def __call__(self, reconstructions, data, mu, lsigma):\n",
        "\n",
        "        # Reconstruction loss\n",
        "        rec_loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "        # KL loss\n",
        "        kl_loss = kl_normal_loss(mu, lsigma)\n",
        "\n",
        "        # Total loss of beta-VAE\n",
        "        loss = rec_loss + self.beta * kl_loss\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. Training the AE MaskGIT model"
      ],
      "metadata": {
        "id": "fDiaYopYzSCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft_3txj0bZjO",
        "outputId": "877ae3c3-026b-4765-f14a-dd91c1e02e20"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training proceeds as usual. I suggest you leave these parameters unchanged for compatibility with the pretrained AE-MaskGIT model that I provide later on, but you can also experiment."
      ],
      "metadata": {
        "id": "01mbSBu1tVpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of AE MaskGIT\n",
        "latent_dim = 10\n",
        "n_code_tokens = 7\n",
        "code_dim = 128\n",
        "codebook_size = 64\n",
        "mlp_hidden_dim = 128\n",
        "decoding_steps = 8"
      ],
      "metadata": {
        "id": "NyZcTZP3a_kc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of MaskTransformer\n",
        "n_patches = 7*7\n",
        "embedding_dim = 128\n",
        "augm_codebook_size = codebook_size+1\n",
        "m_depth = 3\n",
        "m_heads = 4\n",
        "mlp_dim = 256\n",
        "mlp_dropout = 0.1"
      ],
      "metadata": {
        "id": "oB3QvhgO0jYs"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters of CodeTransformer\n",
        "code_dim = 128\n",
        "c_depth = 3\n",
        "c_heads = 4"
      ],
      "metadata": {
        "id": "6sKHkJBM0m7B"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1viDxPQ5z3A9"
      },
      "outputs": [],
      "source": [
        "mask_transformer = MaskTransformer(n_patches=n_patches,\n",
        "                                   n_code_tokens=n_code_tokens,\n",
        "                                   embedding_dim=embedding_dim,\n",
        "                                   codebook_size=augm_codebook_size,\n",
        "                                   depth=m_depth,\n",
        "                                   heads=m_heads,\n",
        "                                   mlp_dim=mlp_dim,\n",
        "                                   mlp_dropout=mlp_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_transformer = CodeTransformer(n_patches=n_patches,\n",
        "                                   n_code_tokens=n_code_tokens,\n",
        "                                   code_dim=code_dim,\n",
        "                                   embedding_dim=embedding_dim,\n",
        "                                   codebook_size=codebook_size,\n",
        "                                   depth=c_depth,\n",
        "                                   heads=c_heads,\n",
        "                                   mlp_dim=mlp_dim,\n",
        "                                   mlp_dropout=mlp_dropout)"
      ],
      "metadata": {
        "id": "_2wvVN5O3g_W"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit = AEMaskGIT(mask_transformer=mask_transformer,\n",
        "                       code_transformer=code_transformer,\n",
        "                       latent_dim=latent_dim,\n",
        "                       n_code_tokens=n_code_tokens,\n",
        "                       code_dim=code_dim,\n",
        "                       codebook_size=codebook_size,\n",
        "                       mlp_dim=mlp_hidden_dim,\n",
        "                       decoding_steps=decoding_steps)"
      ],
      "metadata": {
        "id": "5x_ZhPPK39FU"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit = ae_maskgit.to(device)"
      ],
      "metadata": {
        "id": "xXIZK65V4iY1"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "C-z9KiSzz3A-"
      },
      "outputs": [],
      "source": [
        "vae_loss = BetaVAELoss(beta=1e-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "n_epoch = 10 # if running on GPU you can use more epochs"
      ],
      "metadata": {
        "id": "X0FIYNuj0t1Q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "LVHbow-Cz3A-"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(ae_maskgit.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the loss is stochastic (the amount of tokens masked is random), this means the loss at each iteration is even less likely to decrease monotonically (but there should be a downward trend)."
      ],
      "metadata": {
        "id": "owpkz_0aouVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(tmnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device, GPU or CPU\n",
        "      x = data.to(device)\n",
        "\n",
        "      # Pass the input data through the model\n",
        "      # You can use mode=\"linear\" (and deterministic=False) for instance\n",
        "      logits, mu, lsigma, _ = ae_maskgit(x, mode=\"linear\")\n",
        "\n",
        "      # Compute the beta-VAE loss\n",
        "      loss = vae_loss(logits, x, mu, lsigma)\n",
        "\n",
        "      # Backpropagate\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Aggregate the training loss for display at the end of the epoch\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(tmnist_train_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3853f733-9b69-4512-f794-6f1b1840bb11",
        "id": "2Xfmp063z3A_"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 235/235 [00:14<00:00, 16.69batch/s, loss=90.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 93.7592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [00:13<00:00, 17.01batch/s, loss=91.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 84.9652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 235/235 [00:13<00:00, 16.92batch/s, loss=80.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 83.6913\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 235/235 [00:13<00:00, 16.95batch/s, loss=83.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 83.4201\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 235/235 [00:13<00:00, 16.81batch/s, loss=79]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 82.9550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 235/235 [00:14<00:00, 16.68batch/s, loss=85.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Train Loss: 81.7897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 235/235 [00:14<00:00, 16.48batch/s, loss=75.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Train Loss: 80.9648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 235/235 [00:14<00:00, 16.52batch/s, loss=88.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Train Loss: 79.9477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 235/235 [00:14<00:00, 16.67batch/s, loss=81.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Train Loss: 78.8098\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 235/235 [00:14<00:00, 16.67batch/s, loss=73.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Train Loss: 78.0236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for overfitting\n",
        "\n",
        "We will simply compute the reconstruction loss over the tMNIST test dataset"
      ],
      "metadata": {
        "id": "n1NrHAwP_rbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "with tqdm(tmnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    x = data.to(device)\n",
        "\n",
        "    # Pass the input data through the model\n",
        "    # You can use mode=\"linear\" and deterministic=True for instance\n",
        "    logits, mu, lsigma, _ = ae_maskgit.forward(x, mode=\"linear\", deterministic = True)\n",
        "\n",
        "    # Compute the reconstruction loss\n",
        "    loss = reconstruction_loss(logits, x)\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(tmnist_test_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULzQZ2dQ_1me",
        "outputId": "87a6b6d9-db1a-4529-97a4-304c5794a047"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [00:00<00:00, 49.66batch/s, loss=99.9]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 78.2014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the pretrained model instead"
      ],
      "metadata": {
        "id": "cpocFvv9uVfz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you reached this, it's probably that the training went in the right direction, hence your model is correct. Then you can actually __load an already trained AE-MaskGIT__ if you prefer. If your code is correct and if you kept the same model parameters, it should load without failure."
      ],
      "metadata": {
        "id": "uG_xfOi3s8bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1VbungY0rdQV-K1mZlNA5-8fIC3-qMxSW' -O weights_aemaskgit.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83b97f35-0d19-434d-cdd7-48b2255a3c1a",
        "id": "KQSaNIDMUKda"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2025-03-04 17:23:48--  https://drive.google.com/uc?export=download&id=1VbungY0rdQV-K1mZlNA5-8fIC3-qMxSW\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.102, 142.250.107.100, 142.250.107.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1VbungY0rdQV-K1mZlNA5-8fIC3-qMxSW&export=download [following]\n",
            "--2025-03-04 17:23:48--  https://drive.usercontent.google.com/download?id=1VbungY0rdQV-K1mZlNA5-8fIC3-qMxSW&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 74.125.135.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|74.125.135.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4380410 (4.2M) [application/octet-stream]\n",
            "Saving to: ‘weights_aemaskgit.pth’\n",
            "\n",
            "weights_aemaskgit.p 100%[===================>]   4.18M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-04 17:23:53 (62.5 MB/s) - ‘weights_aemaskgit.pth’ saved [4380410/4380410]\n",
            "\n",
            "FINISHED --2025-03-04 17:23:53--\n",
            "Total wall clock time: 5.0s\n",
            "Downloaded: 1 files, 4.2M in 0.07s (62.5 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dict = torch.load('weights_aemaskgit.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4213b08a-a1a5-4201-e6c9-753a14cf63b1",
        "id": "ucwnQVsBUKda"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-eee59b915dc9>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights_dict = torch.load('weights_aemaskgit.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_EY0VrtNUxKC"
      },
      "outputs": [],
      "source": [
        "mask_transformer = MaskTransformer(n_patches=n_patches,\n",
        "                                   n_code_tokens=n_code_tokens,\n",
        "                                   embedding_dim=embedding_dim,\n",
        "                                   codebook_size=augm_codebook_size,\n",
        "                                   depth=m_depth,\n",
        "                                   heads=m_heads,\n",
        "                                   mlp_dim=mlp_dim,\n",
        "                                   mlp_dropout=mlp_dropout)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "code_transformer = CodeTransformer(n_patches=n_patches,\n",
        "                                   n_code_tokens=n_code_tokens,\n",
        "                                   code_dim=code_dim,\n",
        "                                   embedding_dim=embedding_dim,\n",
        "                                   codebook_size=codebook_size,\n",
        "                                   depth=c_depth,\n",
        "                                   heads=c_heads,\n",
        "                                   mlp_dim=mlp_dim,\n",
        "                                   mlp_dropout=mlp_dropout)"
      ],
      "metadata": {
        "id": "-gfzExteUxKD"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit = AEMaskGIT(mask_transformer=mask_transformer,\n",
        "                       code_transformer=code_transformer,\n",
        "                       latent_dim=latent_dim,\n",
        "                       n_code_tokens=n_code_tokens,\n",
        "                       code_dim=code_dim,\n",
        "                       codebook_size=codebook_size,\n",
        "                       mlp_dim=mlp_hidden_dim,\n",
        "                       decoding_steps=decoding_steps)"
      ],
      "metadata": {
        "id": "7-Lf58ZbUxKD"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit.load_state_dict(weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f10f5d9-124f-40dc-9740-f0d182dff125",
        "id": "ep6I9SLXUKdb"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit = ae_maskgit.to(device)"
      ],
      "metadata": {
        "id": "eWj3GDOquzRB"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Testing AE-MaskGIT"
      ],
      "metadata": {
        "id": "HXmKq33Q6Q9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding - One-Step-Decoding"
      ],
      "metadata": {
        "id": "LTSR1pHKvQjx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will first test AE-MaskGIT in a set-up similar to the training, meaning that we will encode random MNIST images, then learn to __decode them from partially masked image tokens__ (where the masking amount is chosen randomly, similar to training)."
      ],
      "metadata": {
        "id": "NKqtYUPEvWil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_ae1_images(vq_vae, ae_maskgit, test_imgs, device):\n",
        "  '''\n",
        "  Display a batch of input images along with their one-step reconstructions\n",
        "  (after partial token masking)\n",
        "    First row: input images\n",
        "    Second row: reconstructed images\n",
        "  '''\n",
        "\n",
        "  ae_maskgit.eval()\n",
        "\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "\n",
        "  # encode images\n",
        "  img_tokens, _, _ = vq_vae.encode(test_imgs.to(device))\n",
        "\n",
        "  # do a forward pass through ae_maskgit to compute the latent representation, to\n",
        "  # mask tokens and to reconstruct from partially masked tokens\n",
        "  output_logits, _, _, _ = ae_maskgit(img_tokens)\n",
        "  output_tokens = torch.argmax(output_logits, dim=-1)\n",
        "\n",
        "  # decode images from tokens\n",
        "  # Don't forget to pass the output of vq_vae through a sigmoid to get images!\n",
        "  output_imgs = vq_vae.decode(output_tokens)\n",
        "  output_imgs = torch.sigmoid(output_imgs).detach().cpu().numpy()\n",
        "\n",
        "  # print\n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "tFGFz4wI6eyK"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae1_images(vqvae_model, ae_maskgit, test_imgs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "f-V0SKKw9Xyr",
        "outputId": "7db77e5a-e3b8-4174-ade9-4b647673e429"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is reasonably good at the task (we could quantify this if we wanted)."
      ],
      "metadata": {
        "id": "XpmrEOBKwEtF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding - full decoding capabilities"
      ],
      "metadata": {
        "id": "fpXxiaEO9rnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now test the __full encoding - decoding capabilities__ of the framework, by encoding images to a latent vector and reconstructing them purely from this latent vector. In other words, we start from __all masked image tokens__ and we do multi-step image generation conditioned on the latent vector."
      ],
      "metadata": {
        "id": "wrw2YndgwTKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_aef_images(vq_vae, ae_maskgit, test_imgs, device):\n",
        "  '''\n",
        "  Display a batch of input images along with their full reconstructions\n",
        "  purely from the latent code\n",
        "    First row: input images\n",
        "    Second row: reconstructed images\n",
        "  '''\n",
        "\n",
        "  ae_maskgit.eval()\n",
        "\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "\n",
        "  # encode images to tokens\n",
        "  img_tokens, _, _ = vq_vae.encode(test_imgs.to(device))\n",
        "\n",
        "  # encode tokens to latent vector\n",
        "  mu, _ = ae_maskgit.encode(img_tokens)\n",
        "\n",
        "  # reconstruct tokens entirely from the latent vector mu\n",
        "  # I suggest you set deterministic=True\n",
        "  output_tokens = ae_maskgit.decode_test(mu, deterministic = True)\n",
        "\n",
        "  # decode images from tokens\n",
        "  # Don't forget to pass the output of vq_vae through a sigmoid to get images!\n",
        "  output_imgs = vq_vae.decode(output_tokens)\n",
        "  output_imgs = torch.sigmoid(output_imgs).detach().cpu().numpy()\n",
        "\n",
        "  # print\n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dye8zpIQ-Das"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstructing test images\n",
        "ae_maskgit.steps = 20\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_aef_images(vqvae_model, ae_maskgit, test_imgs, device)\n",
        "ae_maskgit.steps = decoding_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "id": "RQkN0VRF-Dat",
        "outputId": "3810670b-c816-46b0-85a5-80bf4418359d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is decent at this. We can quantify this below. It does not reconstruct perfectly the digits (the autoencoding mechanism is a bit naive + it is a very small transformer), but compared to a vanilla VAE there is little blurring in the reconstructions. The reconstructions are semantically correct but there are a few perceptual mistakes."
      ],
      "metadata": {
        "id": "qrzgTQ1hwyMB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute quantitative reconstruction metrics\n",
        "\n",
        "We will compute the binary cross entropy in image space."
      ],
      "metadata": {
        "id": "C6C3BFuKAaaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit.eval()\n",
        "vqvae_model.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "ae_maskgit.steps = 8\n",
        "\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    x = data.to(device)\n",
        "\n",
        "    # encode images to tokens\n",
        "    img_tokens, _, _ = vqvae_model.encode(x)\n",
        "\n",
        "    # encode tokens to latent vector\n",
        "    mu, _ = ae_maskgit.encode(img_tokens)\n",
        "\n",
        "    # decode latent vector to tokens\n",
        "    output_tokens = ae_maskgit.decode_test(mu, mode=\"linear\", temperature=1., deterministic=True)\n",
        "\n",
        "    # decode images from tokens\n",
        "    x_hat = vqvae_model.decode(output_tokens)\n",
        "\n",
        "    # Compute the reconstruction loss\n",
        "    # Hint: x_hat contains logits!\n",
        "    loss = F.binary_cross_entropy_with_logits(x_hat, x, reduction=\"mean\")\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))\n",
        "\n",
        "ae_maskgit.steps = decoding_steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5awb6Nl8Akj5",
        "outputId": "da2e9aa1-613b-431d-fdb8-21b6c6f7abfd"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 23.04batch/s, loss=0.118]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.1231\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can directly compare this number (0.1231 if you have the same model and code as I do) to the one we had obtained when simply autoencoding through the VQ-VAE part, which was 0.0713. The fact that the reconstructed digits are not blurry, but that there are a few perceptual reconstruction errors means that this binary cross-entropy metric is naturally a bit higher than it could be."
      ],
      "metadata": {
        "id": "L9DOAAFCyFGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5. Generative capabilities of AE-MaskGIT"
      ],
      "metadata": {
        "id": "o7rF3lXzBr7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have not yet tried to generate images from AE-MaskGIT (we've only done auto-encoding so far). To __generate images__, we should generate a latent vector from the prior $p(z)=N(z;0,1)$, then predict tokens from this latent vector (this part is done via `ae_maskgit.generate_samples()`) then decode the tokens back to image space (this part is done via `vqvae_model.decode()`)."
      ],
      "metadata": {
        "id": "XYz1Pbbcyi4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_aemaskgit(vq_vae, ae_maskgit, n_images=5):\n",
        "    output_tokens = ae_maskgit.generate_samples(N=n_images, mode=\"cosine\", temperature=10., randomize=\"warm_up\", deterministic=True, r=None)\n",
        "    x_hat = vqvae_model.decode(output_tokens)\n",
        "    return torch.sigmoid(x_hat)"
      ],
      "metadata": {
        "id": "lvrE4nBWB01n"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhTA8AStB01m"
      },
      "source": [
        "Now, generate some images. You can rerun the cell several times for more variety. (I am optionally increasing the number of decoding steps in AE-MaskGIT, to potentially improve the quality of samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "tgnzxSY7B01o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "28586075-7b6c-48c8-fc02-35e993d03e99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "ae_maskgit.steps = 20\n",
        "\n",
        "imgs_generated = generate_images_aemaskgit(vqvae_model, ae_maskgit)\n",
        "display_images(imgs_generated)\n",
        "\n",
        "ae_maskgit.steps = decoding_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__What do you think of these samples?__ Some are good, some are not so good. Let's try next to improve the quality of generated samples without fundamentally changing our model!"
      ],
      "metadata": {
        "id": "Jh_z6uPUzeE3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Generating better samples via Noise Contrastive Priors!"
      ],
      "metadata": {
        "id": "x9OQy7WX4NJg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the course, we have seen that a common issue of VAE-like models is the prior hole problem i.e., the __discrepancy between the prior and the aggregate posterior__ $p(z)$ and $q(z)$. We can try to fix that with the NCP-VAE strategy described in the course, which attempts to modify the prior after training the model $\\tilde{p}(z)\\propto p(z)r(z)$ to make it match the aggregate posterior $q(z)$ more closely.<br>\n",
        "\n",
        "The closed-form solution of minimizing $KL(q(z)||\\tilde{p}(z))$ w.r.t. $r$ yields $r(z):= q(z)/p(z)$, a quantity which can be estimated by the likelihood ratio trick. Specifically, we will train a discriminator network `D` (a small MLP) to discriminate samples from $q(z)$ (class 1) from samples from $p(z)=N(0,1)$ (class 0). After training the model, the logit `D(z)` output by the neural network is an estimate of $\\log{q(z)/p(z)}$, so that $r(z)=\\exp{D(z)}$.<br>\n",
        "\n",
        "All that is left to do is to be able to sample from $\\tilde{p}(z)\\propto p(z)r(z)$. We will use sampling-importance-resampling, which proposes to\n",
        "1.   sample from $p(z)$ `n_candidates` samples `z_i`\n",
        "2.   compute `w_i=r(z_i)`\n",
        "3.   sample an index `i` among the `n_candidates` with probability proportional to `w_i`\n",
        "\n",
        "These three steps give us an approximate sample from $\\tilde{p}(z)$.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o2ypK6Mp07l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set out to implement this next!"
      ],
      "metadata": {
        "id": "4KXUfjH33kFa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1. Generating a dataset of samples from $q(z)$"
      ],
      "metadata": {
        "id": "oUNkX04k31px"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The samples from $q(z)$ needed to train `D` are obtained by sampling randomly an image from MNIST trainset and computing the corresponding latent vector. Instead of doing it on the fly at each training iteration, which is fairly costly, we will precompute all the latent vectors in the `cMNIST` dataset, like we did previously with `tMNIST`."
      ],
      "metadata": {
        "id": "rt_kgE433_WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class cMNIST(object):\n",
        "    def __init__(self, ae_maskgit, tmnist_loader, device):\n",
        "        ae_maskgit = ae_maskgit.to(device)\n",
        "        self.data = self.process(ae_maskgit, tmnist_loader, device)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.data[i]\n",
        "\n",
        "    def process(self, ae_maskgit, tmnist_loader, device):\n",
        "        ae_maskgit.eval()\n",
        "        latents = []\n",
        "\n",
        "        with tqdm(tmnist_loader, unit=\"batch\") as tepoch:\n",
        "            for data in tepoch:\n",
        "                # Put the data on the correct device:\n",
        "                data = data.to(device)\n",
        "\n",
        "                # Pass the data through the model\n",
        "                mu, _ = ae_maskgit.encode(data)\n",
        "                latents.append(mu.detach().clone().cpu().numpy())\n",
        "\n",
        "        return np.concatenate(latents, axis=0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return np.shape(self.data)[0]"
      ],
      "metadata": {
        "id": "DJY4_Wd76TX-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmnist_trainset = cMNIST(ae_maskgit, tmnist_train_loader, device)\n",
        "cmnist_testset = cMNIST(ae_maskgit, tmnist_test_loader, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785e2692-3d0e-45cc-ad08-5d3a34a983a4",
        "id": "dKbGqlbZ6TYA"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 235/235 [00:02<00:00, 98.60batch/s]\n",
            "100%|██████████| 40/40 [00:00<00:00, 98.48batch/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "\n",
        "cmnist_train_loader = torch.utils.data.DataLoader(cmnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "cmnist_test_loader = torch.utils.data.DataLoader(cmnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "QhdN8Dww6TYB"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7408e61-c2f7-456b-d3d0-44555c8ab2c0",
        "id": "ZVtra1A76TYC"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "cmnist_trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2. Building and training D"
      ],
      "metadata": {
        "id": "Ac7y-4v14nkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use as loss the `binary_cross_entropy_with_logits`, assigning the target for samples from $q$ as 1 and the target for samples from $p$ as 0."
      ],
      "metadata": {
        "id": "bB7ppo7V50mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discriminator_loss(D, z_q, z_p):\n",
        "    y_f = D(z_q)\n",
        "    y_r = D(z_p)\n",
        "\n",
        "    loss = 0.5 * (\n",
        "            F.binary_cross_entropy_with_logits(y_f, torch.ones_like(y_f)) +\n",
        "            F.binary_cross_entropy_with_logits(y_r, torch.zeros_like(y_r))\n",
        "                )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "uSEH1JtfaZ0U",
        "execution": {
          "iopub.status.busy": "2024-05-10T19:10:45.770193Z",
          "iopub.execute_input": "2024-05-10T19:10:45.770550Z",
          "iopub.status.idle": "2024-05-10T19:10:45.776515Z",
          "shell.execute_reply.started": "2024-05-10T19:10:45.770522Z",
          "shell.execute_reply": "2024-05-10T19:10:45.774989Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": 83
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `discriminator_model` maps from a latent code of dimension `latent_dim` to a single logit (-> probability to be a sample from $q(z)$).<br>\n",
        "\n",
        "Build a model with:\n",
        "1.   A linear layer that maps to a vector of size 128\n",
        "2.   An ELU nonlinearity\n",
        "3.   A LayerNorm\n",
        "4.   A linear layer with 128 output neurons\n",
        "5.   An ELU nonlinearity\n",
        "6.   A LayerNorm\n",
        "7.   A linear layer that produces the output\n",
        "\n"
      ],
      "metadata": {
        "id": "7XVN-T6S4zDN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model = nn.Sequential(\n",
        "          nn.Linear(latent_dim, 128),\n",
        "          nn.ELU(),\n",
        "          nn.LayerNorm(128),\n",
        "          nn.Linear(128, 128),\n",
        "          nn.ELU(),\n",
        "          nn.LayerNorm(128),\n",
        "          nn.Linear(128, 1)\n",
        "        )"
      ],
      "metadata": {
        "id": "nf8H5E027Myl"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model = discriminator_model.to(device)"
      ],
      "metadata": {
        "id": "-N1jpRh5RyJ_"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's train the model."
      ],
      "metadata": {
        "id": "pUMsnxk36Nn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_epoch_d = 10\n",
        "learning_rate_d = 1e-3\n",
        "optimizer_d = optim.AdamW(discriminator_model.parameters(), lr=learning_rate_d, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "e9ke9V7Z8fN-"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch_d):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(cmnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put data on correct device, GPU or CPU\n",
        "      z_q = data.to(device)\n",
        "\n",
        "      # Generate z_p from p(z)\n",
        "      z_p = ae_maskgit.sample_pz(z_q.size(0))\n",
        "\n",
        "      # Compute the discriminator loss\n",
        "      loss = discriminator_loss(discriminator_model, z_q, z_p)\n",
        "\n",
        "      # Backpropagate\n",
        "      optimizer_d.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer_d.step()\n",
        "\n",
        "      # Aggregate the training loss for display at the end of the epoch\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # tqdm bar displays the loss\n",
        "      tepoch.set_postfix(losses=loss.item())\n",
        "\n",
        "  print('Epoch {}: Discriminator Loss: {:.4f}'.format(epoch, train_loss/len(cmnist_train_loader)))"
      ],
      "metadata": {
        "id": "dLP-6fYo83A5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e301167a-abf7-4958-b3c9-4309628a4560"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 235/235 [00:01<00:00, 121.74batch/s, losses=0.598]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Discriminator Loss: 0.6467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 235/235 [00:01<00:00, 131.32batch/s, losses=0.625]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Discriminator Loss: 0.5981\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 235/235 [00:01<00:00, 136.08batch/s, losses=0.572]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Discriminator Loss: 0.5574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 235/235 [00:01<00:00, 128.92batch/s, losses=0.489]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Discriminator Loss: 0.5239\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 235/235 [00:01<00:00, 133.07batch/s, losses=0.488]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Discriminator Loss: 0.4998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 235/235 [00:01<00:00, 132.04batch/s, losses=0.465]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: Discriminator Loss: 0.4833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 235/235 [00:02<00:00, 105.93batch/s, losses=0.481]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: Discriminator Loss: 0.4736\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 235/235 [00:01<00:00, 123.38batch/s, losses=0.475]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: Discriminator Loss: 0.4650\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 235/235 [00:01<00:00, 130.50batch/s, losses=0.488]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: Discriminator Loss: 0.4567\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 235/235 [00:01<00:00, 138.02batch/s, losses=0.44]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: Discriminator Loss: 0.4550\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If it has trained properly, you can load a pretrained discriminator model instead if you want (that is only valid for the pretrained AE-MaskGIT model!)"
      ],
      "metadata": {
        "id": "9l2IsLYg6fVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1kCFZXzW3uMaVjUIBTfa8yNJ79PgLj95Y' -O weights_discriminator.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9645c3-247f-4b0a-e22f-385f70cce359",
        "id": "QpoGWBqfRy75"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2025-03-04 17:24:22--  https://drive.google.com/uc?export=download&id=1kCFZXzW3uMaVjUIBTfa8yNJ79PgLj95Y\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.113, 142.250.107.139, 142.250.107.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1kCFZXzW3uMaVjUIBTfa8yNJ79PgLj95Y&export=download [following]\n",
            "--2025-03-04 17:24:23--  https://drive.usercontent.google.com/download?id=1kCFZXzW3uMaVjUIBTfa8yNJ79PgLj95Y&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.98.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.98.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78210 (76K) [application/octet-stream]\n",
            "Saving to: ‘weights_discriminator.pth’\n",
            "\n",
            "weights_discriminat 100%[===================>]  76.38K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-03-04 17:24:25 (85.7 MB/s) - ‘weights_discriminator.pth’ saved [78210/78210]\n",
            "\n",
            "FINISHED --2025-03-04 17:24:25--\n",
            "Total wall clock time: 2.4s\n",
            "Downloaded: 1 files, 76K in 0.001s (85.7 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dict = torch.load('weights_discriminator.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad56185-912e-4b01-968a-f9feaafb4936",
        "id": "OKTTzzCrRy77"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-0083c9149c91>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights_dict = torch.load('weights_discriminator.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model.load_state_dict(weights_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s6ET1RvRyY5",
        "outputId": "65a604f3-c0dd-400b-c069-096e59513030"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator_model = discriminator_model.to(device)"
      ],
      "metadata": {
        "id": "JoyQlnac7Ykw"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3. Using the noise contrastive prior"
      ],
      "metadata": {
        "id": "oJMOIOIk7A5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build a small class that stores the trained discriminator and computes $r(z)$ given $z$ when it is called."
      ],
      "metadata": {
        "id": "EF_nXXIo7FFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class r(object):\n",
        "    def __init__(self, discriminator):\n",
        "        self.D = discriminator\n",
        "    def __call__(self, z):\n",
        "        z = z.to(device)\n",
        "        return torch.exp(self.D(z)).squeeze(-1)"
      ],
      "metadata": {
        "id": "DCbuhR91a9nw",
        "execution": {
          "iopub.status.busy": "2024-05-10T19:10:45.972049Z",
          "iopub.execute_input": "2024-05-10T19:10:45.972870Z",
          "iopub.status.idle": "2024-05-10T19:10:45.977553Z",
          "shell.execute_reply.started": "2024-05-10T19:10:45.972828Z",
          "shell.execute_reply": "2024-05-10T19:10:45.976481Z"
        },
        "trusted": true
      },
      "outputs": [],
      "execution_count": 92
    },
    {
      "cell_type": "code",
      "source": [
        "r_model = r(discriminator_model)"
      ],
      "metadata": {
        "id": "ZvhUqhhmEPVE"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All that is left is to complete the routine `sir` in the `AEMaskGIT` class if you have not done so yet. It implements the sampling importance resampling.<br>\n",
        "\n",
        "After modifying the class `AEMaskGIT`, you may have to reinitialize the model for changes to be taken into account and to repopulate it with the pretrained weights. Then, you can run the cells below, which draw samples from the model using the NCP prior!"
      ],
      "metadata": {
        "id": "UaHfhVx97Qxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_aemaskgit(vq_vae, ae_maskgit, n_images=5, r=None):\n",
        "    output_tokens = ae_maskgit.generate_samples(N=n_images, mode=\"cosine\", temperature=10., randomize=\"warm_up\", deterministic=True, r=r)\n",
        "    x_hat = vqvae_model.decode(output_tokens)\n",
        "    return torch.sigmoid(x_hat)"
      ],
      "metadata": {
        "id": "aoglBl_IDGDA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "aZPUnb7ODGDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "72c9b9ff-64d9-415b-9359-582b6d56c675"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "ae_maskgit.steps = 20\n",
        "\n",
        "imgs_generated = generate_images_aemaskgit(vqvae_model, ae_maskgit, n_images=5, r=r_model)\n",
        "display_images(imgs_generated)\n",
        "\n",
        "ae_maskgit.steps = decoding_steps"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The samples should now be qualitatively much better on average! In the final part of this lab, where you have nothing to code, we will quantify this."
      ],
      "metadata": {
        "id": "DwbQtF_f78Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Evaluating the quality of generated samples"
      ],
      "metadata": {
        "id": "DAa8vdKuHHoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA_1GT9898zg",
        "outputId": "07136f6d-8ba2-4185-f86c-98c0ed6ec013"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most widely accepted metric for evaluating the quality of generated images is the FID (Fréchet Inception Distance) which we have discussed in the course. You could implement it if you are interested, but we will opt for a simpler (and more naive) metric."
      ],
      "metadata": {
        "id": "jMKvIQkU8OxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train a digit classifier on real MNIST samples. As the task is fairly easy, we will reach close to 100% accuracy. We will assume for this reason that a good digit is an image that can be easily classified into one of the 10 digits: in other words, an image for which the maximum of the 10 output probabilities is close to 1.<br>\n",
        "\n",
        "We will hence __measure the generation quality__ as the average over generated images, of the maximum probability (computed separately for each generated image) $p_{i*}=\\max_{i=1\\dots 10} p_i$. The upper bound of the metric will be the test-time classification accuracy on real-digits, as fake digits will probably be harder to classify."
      ],
      "metadata": {
        "id": "qQc2bS8b8jlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = 10\n",
        "nb_filters = 32\n",
        "kernel_size = (3, 3)\n",
        "pool_size = (2, 2)\n",
        "\n",
        "mnist_classification_model = torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(1, 32, kernel_size=kernel_size, stride=2, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.BatchNorm2d(32),\n",
        "        torch.nn.Conv2d(32, 64, kernel_size=kernel_size, stride=2, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.BatchNorm2d(64),\n",
        "        torch.nn.Conv2d(64, 128, kernel_size=kernel_size, stride=2, padding=1),\n",
        "        torch.nn.ReLU(),\n",
        "        torch.nn.BatchNorm2d(128),\n",
        "        torch.nn.AvgPool2d(kernel_size=(4,4), stride=4),\n",
        "        torch.nn.Flatten(),\n",
        "        torch.nn.Linear(128, nb_classes)\n",
        "    )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:27.387638Z",
          "iopub.execute_input": "2024-05-10T19:15:27.388070Z",
          "iopub.status.idle": "2024-05-10T19:15:27.398972Z",
          "shell.execute_reply.started": "2024-05-10T19:15:27.388034Z",
          "shell.execute_reply": "2024-05-10T19:15:27.398154Z"
        },
        "trusted": true,
        "id": "VXXULJEPHKoS"
      },
      "outputs": [],
      "execution_count": 97
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate -r 'https://drive.google.com/uc?export=download&id=1s9naPzcS_uOzX4Hhn9LxWEew7okv2Xg6' -O weights_classif.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ca376a-22b8-47eb-eb58-5e0e4d0e4d24",
        "id": "ZWI0QSawHa4F"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: combining -O with -r or -p will mean that all downloaded content\n",
            "will be placed in the single file you specified.\n",
            "\n",
            "--2025-03-04 17:24:25--  https://drive.google.com/uc?export=download&id=1s9naPzcS_uOzX4Hhn9LxWEew7okv2Xg6\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.107.113, 142.250.107.139, 142.250.107.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.107.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1s9naPzcS_uOzX4Hhn9LxWEew7okv2Xg6&export=download [following]\n",
            "--2025-03-04 17:24:25--  https://drive.usercontent.google.com/download?id=1s9naPzcS_uOzX4Hhn9LxWEew7okv2Xg6&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.98.132, 2607:f8b0:400e:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.98.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 386982 (378K) [application/octet-stream]\n",
            "Saving to: ‘weights_classif.pth’\n",
            "\n",
            "weights_classif.pth 100%[===================>] 377.91K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-03-04 17:24:28 (91.0 MB/s) - ‘weights_classif.pth’ saved [386982/386982]\n",
            "\n",
            "FINISHED --2025-03-04 17:24:28--\n",
            "Total wall clock time: 2.9s\n",
            "Downloaded: 1 files, 378K in 0.004s (91.0 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weights_dict = torch.load('weights_classif.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2def3114-2a1b-476b-f018-1144969a7701",
        "id": "5NUuWXhaHa4H"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-99-8f9053c268ef>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  weights_dict = torch.load('weights_classif.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_classification_model.load_state_dict(weights_dict)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:27.657658Z",
          "iopub.execute_input": "2024-05-10T19:15:27.657946Z",
          "iopub.status.idle": "2024-05-10T19:15:27.698131Z",
          "shell.execute_reply.started": "2024-05-10T19:15:27.657921Z",
          "shell.execute_reply": "2024-05-10T19:15:27.697341Z"
        },
        "trusted": true,
        "id": "aBn2nr7q2J9K",
        "outputId": "96da4ecc-12c3-466d-e7ab-8386785b9684",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "execution_count": 100
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_classification_model = mnist_classification_model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:28.179026Z",
          "iopub.execute_input": "2024-05-10T19:15:28.179617Z",
          "iopub.status.idle": "2024-05-10T19:15:28.185754Z",
          "shell.execute_reply.started": "2024-05-10T19:15:28.179589Z",
          "shell.execute_reply": "2024-05-10T19:15:28.184801Z"
        },
        "trusted": true,
        "id": "kxUsWOFu2J9K"
      },
      "outputs": [],
      "execution_count": 101
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function to calculate accuracy, instead of loss"
      ],
      "metadata": {
        "id": "_sUZONxw2bnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n",
        "  return y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:37.094394Z",
          "iopub.execute_input": "2024-05-10T19:15:37.094767Z",
          "iopub.status.idle": "2024-05-10T19:15:37.099666Z",
          "shell.execute_reply.started": "2024-05-10T19:15:37.094738Z",
          "shell.execute_reply": "2024-05-10T19:15:37.098590Z"
        },
        "trusted": true,
        "id": "u8-ECeUjHKoW"
      },
      "outputs": [],
      "execution_count": 102
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_accuracy(x_pred,x_label):\n",
        "  acc = (x_pred == x_label).sum()/(x_pred.shape[0])\n",
        "  return acc"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:37.653508Z",
          "iopub.execute_input": "2024-05-10T19:15:37.653830Z",
          "iopub.status.idle": "2024-05-10T19:15:37.658636Z",
          "shell.execute_reply.started": "2024-05-10T19:15:37.653804Z",
          "shell.execute_reply": "2024-05-10T19:15:37.657689Z"
        },
        "trusted": true,
        "id": "gPrdtzhjHKoW"
      },
      "outputs": [],
      "execution_count": 103
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_classification_model.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for imgs, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    predict=mnist_classification_model(imgs)\n",
        "    all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "test_accuracy = cnn_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ad14f3-f3d4-4af7-d9d1-80bc313cd08e",
        "execution": {
          "iopub.status.busy": "2024-05-10T19:15:41.443095Z",
          "iopub.execute_input": "2024-05-10T19:15:41.443456Z",
          "iopub.status.idle": "2024-05-10T19:15:42.770067Z",
          "shell.execute_reply.started": "2024-05-10T19:15:41.443428Z",
          "shell.execute_reply": "2024-05-10T19:15:42.769212Z"
        },
        "trusted": true,
        "id": "WLSZM5xeHKoY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:01<00:00, 105.04batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": 104
    },
    {
      "cell_type": "markdown",
      "source": [
        "0.983 will be our upper bound for the generative score. Let's code the __generative score__ as described above:"
      ],
      "metadata": {
        "id": "5KdS_hrH-Gp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generative_model_score(imgs_in,classification_model):\n",
        "  gen_score = torch.mean(torch.max(F.softmax(classification_model(imgs_in),dim=1),axis=1)[0])\n",
        "  return(gen_score)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:45:07.969516Z",
          "iopub.execute_input": "2024-05-10T19:45:07.970215Z",
          "iopub.status.idle": "2024-05-10T19:45:07.975098Z",
          "shell.execute_reply.started": "2024-05-10T19:45:07.970181Z",
          "shell.execute_reply": "2024-05-10T19:45:07.974128Z"
        },
        "trusted": true,
        "id": "livYeWrrH9of"
      },
      "outputs": [],
      "execution_count": 105
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, generate some images with and without the NCP prior, and evaluate these models (there still is some variability in the score even with 2000 images, don't hesitate to rerun the cell several times):"
      ],
      "metadata": {
        "id": "MjglYMX3H9og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_maskgit.steps = 20\n",
        "imgs_model_NCP = generate_images_aemaskgit(vqvae_model, ae_maskgit, n_images=2000, r=r_model)\n",
        "imgs_model = generate_images_aemaskgit(vqvae_model, ae_maskgit, n_images=2000, r=None)\n",
        "ae_maskgit.steps = decoding_steps\n",
        "\n",
        "model_score_NCP = float(generative_model_score(imgs_model_NCP,mnist_classification_model))\n",
        "model_score = float(generative_model_score(imgs_model,mnist_classification_model))\n",
        "\n",
        "print('Upper bound: {}'.format(0.983))\n",
        "print('Score without NCP: {}'.format(model_score))\n",
        "print('Score with NCP: {}'.format(model_score_NCP))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-10T19:45:22.070514Z",
          "iopub.execute_input": "2024-05-10T19:45:22.071263Z",
          "iopub.status.idle": "2024-05-10T19:45:22.136975Z",
          "shell.execute_reply.started": "2024-05-10T19:45:22.071232Z",
          "shell.execute_reply": "2024-05-10T19:45:22.136104Z"
        },
        "trusted": true,
        "id": "NNVM88woH9oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e468032-2797-40ac-d21b-31cae55c467b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upper bound: 0.983\n",
            "Score without NCP: 0.9274080991744995\n",
            "Score with NCP: 0.9750217795372009\n"
          ]
        }
      ],
      "execution_count": 106
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score of the model with NCP is pretty good (around 0.975-0.98). The goal for this lab was not to train the best possible model (as decent results can be achieved with simpler models), but rather to learn a range of techniques while training a decent model."
      ],
      "metadata": {
        "id": "-fr6vOzv_pfh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__ps.__ Can you think of an __issue__ with the metric we used as a __generative score__, which would actually be more of an __issue for evaluating GAN models__?"
      ],
      "metadata": {
        "id": "1uiSzJpaAlVC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The issue with the generative score metric is that it evaluates quality based on how well a classifier can recognize generated images instead of evaluating the similitude between the generated and training sets. It presents limitations, particularly when evaluating GANs or models generating more complex data. Indeed the generation of realistic images, which may not be easily classified. As a result, the score might unfairly penalize high-quality images that are hard for the classifier to label."
      ],
      "metadata": {
        "id": "M7elzLOSP0ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#References\n",
        "\n",
        "Part of this notebook was adapted from Michael Alcorn's (airalcorn2's) VQ-VAE pytorch implementation under the following [Apache-2.0 license](https://github.com/airalcorn2/vqvae-pytorch/tree/master?tab=Apache-2.0-1-ov-file#readme). Another part was loosely adapted from valeo.ai's MaskGIT implementation under the following [MIT license](https://github.com/valeoai/Maskgit-pytorch/tree/main)."
      ],
      "metadata": {
        "id": "gyIB4jnzoL48"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}